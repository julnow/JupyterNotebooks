{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables of filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background comes from URQMD, signal from DCM, for test dataset for both bckgr and sign generated in urqmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4:1 background/signal ratio before cleaning, 5M sign, 15k bckgr, 30k ALL, nochi2geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLAINTREE FILES NAMES\n",
    "directory='/Users/julnow/gsi/cbm/ML/JupyterNotebooks/'\n",
    "signalFileName = directory + 'PlainTree_5MSign.root'\n",
    "allURQMD = directory + 'PlainTree_30kAll_URQMD.root'\n",
    "allDCM = directory + 'PlainTree_20kAll_DCM.root'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wXUjK-nyW4xt"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!git clone https://github.com/shahidzk1/CBM_ML_Lambda_Library.git\n",
    "%cd CBM_ML_Lambda_Library\n",
    "!git pull origin main\n",
    "!pip install -r requirements.txt\n",
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UXNBDcG472oy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n",
    "#from hipe4ml.model_handler import ModelHandler\n",
    "#from hipe4ml.tree_handler import TreeHandler\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "#from hipe4ml import plot_utils\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import scipy\n",
    "from scipy.stats import uniform\n",
    "\n",
    "from numpy import sqrt, log, argmax\n",
    "\n",
    "import weakref \n",
    "import itertools\n",
    "\n",
    "from CBM_ML import tree_importer,  KFPF_lambda_cuts, plot_tools\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt import SequentialDomainReductionTransformer\n",
    "\n",
    "import gc, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4Ie_PTM72qH"
   },
   "source": [
    "# Selecting Background and Signal\n",
    "We generate PlainTrees from: 2.5M events (signal) and 30k evenents (background) with Au-Au @12A GeV/c, DCMQGSM-SMM generator passed through CBM setup in GEANT4, without any cuts\n",
    "\n",
    "To omit imbalance classification problem we resample the data. We're deleting instances from the over-represented class (in our case the background) - it's called under-sampling, one of the resampling methods. \n",
    "\n",
    "So for training and testing we will get signal candidates from the 5 sigma region and  background from outside this region (4 times signal size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#CUTS FOR MASS SELECTION\n",
    "#5 sigma region for signal\n",
    "lower5SigmaCutSign = 0.43485\n",
    "upper5SigmaCutSign = 0.56135\n",
    "#mean invariant mass\n",
    "invMass = 0.4981\n",
    "# \"5sigma\" (not acutal 5 sigma) region for background\n",
    "lower5SigmaCutBckgr = 0.279\n",
    "upper5SigmaCutBckgr = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wKnmnX_TwAig"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2506"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We import three root files into our jupyter notebook\n",
    "#sign - before cleaning but after 5sigma seletion, signal - cleaned\n",
    "#so that we dont have to reimport everything with each change\n",
    "sign = tree_importer.tree_importer(signalFileName,'PlainTree',7)\n",
    "\n",
    "# We only select lambda candidates in the 5 sigma region around the kaon mass peak\n",
    "#we preserve the cleaned dataframe with a changed name\n",
    "sign = sign[(sign['Candidates_generation']==1) & (sign['Candidates_mass']>lower5SigmaCutSign) & (sign['Candidates_mass']<upper5SigmaCutSign)]\n",
    "\n",
    "# Similarly for the background, we select background candidates which are not in the 5 sigma region of the kaon peak\n",
    "bckgr = tree_importer.tree_importer(allURQMD,'PlainTree',7)\n",
    "#we preserve the cleaned dataframe with a changed name\n",
    "bckgr = bckgr[(bckgr['Candidates_generation'] < 1)\n",
    "                 & ((bckgr['Candidates_mass'] > lower5SigmaCutBckgr)\n",
    "                 & (bckgr['Candidates_mass'] < lower5SigmaCutSign) | (bckgr['Candidates_mass']>upper5SigmaCutSign) \n",
    "                    & (bckgr['Candidates_mass'] < upper5SigmaCutBckgr))\n",
    "             ].sample(n=4*(sign.shape[0])) #we select bckgr so that we have 4*more entries that for signal (before cleaning)\n",
    "\n",
    "#Also call the garbage collector of python to collect unused items to free memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chi2geo</th>\n",
       "      <th>chi2primfirst</th>\n",
       "      <th>chi2primsecond</th>\n",
       "      <th>chi2topo</th>\n",
       "      <th>cosinefirst</th>\n",
       "      <th>cosinesecond</th>\n",
       "      <th>cosinetopo</th>\n",
       "      <th>distance</th>\n",
       "      <th>eta</th>\n",
       "      <th>l</th>\n",
       "      <th>...</th>\n",
       "      <th>pT</th>\n",
       "      <th>phi</th>\n",
       "      <th>px</th>\n",
       "      <th>py</th>\n",
       "      <th>pz</th>\n",
       "      <th>rapidity</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>issignal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.055125</td>\n",
       "      <td>1.191851e+03</td>\n",
       "      <td>2.418227e+03</td>\n",
       "      <td>1.382537</td>\n",
       "      <td>0.992775</td>\n",
       "      <td>0.978738</td>\n",
       "      <td>0.999984</td>\n",
       "      <td>0.010199</td>\n",
       "      <td>1.767213</td>\n",
       "      <td>0.831264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.858095</td>\n",
       "      <td>-1.419017</td>\n",
       "      <td>0.129742</td>\n",
       "      <td>-0.848230</td>\n",
       "      <td>2.438579</td>\n",
       "      <td>1.629621</td>\n",
       "      <td>-0.138814</td>\n",
       "      <td>-0.281002</td>\n",
       "      <td>0.784715</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.675698</td>\n",
       "      <td>1.191851e+03</td>\n",
       "      <td>2.562767e+00</td>\n",
       "      <td>1.478705</td>\n",
       "      <td>0.992461</td>\n",
       "      <td>0.979261</td>\n",
       "      <td>0.995678</td>\n",
       "      <td>0.176995</td>\n",
       "      <td>1.760604</td>\n",
       "      <td>0.462111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.872880</td>\n",
       "      <td>-1.416307</td>\n",
       "      <td>0.134315</td>\n",
       "      <td>-0.862485</td>\n",
       "      <td>2.463272</td>\n",
       "      <td>1.624846</td>\n",
       "      <td>-0.118300</td>\n",
       "      <td>-0.178443</td>\n",
       "      <td>0.427863</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.884655</td>\n",
       "      <td>7.708000e+05</td>\n",
       "      <td>6.868500e+03</td>\n",
       "      <td>0.525167</td>\n",
       "      <td>0.986342</td>\n",
       "      <td>0.971431</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.621583</td>\n",
       "      <td>3.087167</td>\n",
       "      <td>6.618644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189899</td>\n",
       "      <td>-2.975034</td>\n",
       "      <td>-0.187271</td>\n",
       "      <td>-0.031483</td>\n",
       "      <td>2.076483</td>\n",
       "      <td>2.065397</td>\n",
       "      <td>-0.803517</td>\n",
       "      <td>-0.114543</td>\n",
       "      <td>6.589974</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005968</td>\n",
       "      <td>7.835867e+03</td>\n",
       "      <td>6.868500e+03</td>\n",
       "      <td>1.604101</td>\n",
       "      <td>0.986588</td>\n",
       "      <td>0.972632</td>\n",
       "      <td>0.999912</td>\n",
       "      <td>0.122127</td>\n",
       "      <td>3.098664</td>\n",
       "      <td>6.632589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185934</td>\n",
       "      <td>-2.957098</td>\n",
       "      <td>-0.182778</td>\n",
       "      <td>-0.034110</td>\n",
       "      <td>2.056715</td>\n",
       "      <td>2.062181</td>\n",
       "      <td>-0.830276</td>\n",
       "      <td>-0.054652</td>\n",
       "      <td>6.602469</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.048226</td>\n",
       "      <td>1.085611e+07</td>\n",
       "      <td>9.202689e+05</td>\n",
       "      <td>0.103476</td>\n",
       "      <td>0.928590</td>\n",
       "      <td>0.993457</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>2.737351</td>\n",
       "      <td>7.693739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224500</td>\n",
       "      <td>2.629986</td>\n",
       "      <td>-0.195755</td>\n",
       "      <td>0.109911</td>\n",
       "      <td>1.726547</td>\n",
       "      <td>1.875214</td>\n",
       "      <td>-1.039354</td>\n",
       "      <td>0.486576</td>\n",
       "      <td>7.631403</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.564517</td>\n",
       "      <td>1.755055e+06</td>\n",
       "      <td>3.325792e+06</td>\n",
       "      <td>3.620159</td>\n",
       "      <td>0.986431</td>\n",
       "      <td>0.917482</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.005039</td>\n",
       "      <td>2.155051</td>\n",
       "      <td>6.028908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.337338</td>\n",
       "      <td>0.300585</td>\n",
       "      <td>0.322212</td>\n",
       "      <td>0.099879</td>\n",
       "      <td>1.435782</td>\n",
       "      <td>1.604478</td>\n",
       "      <td>1.300065</td>\n",
       "      <td>0.455394</td>\n",
       "      <td>5.872846</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.265824</td>\n",
       "      <td>1.272178e+06</td>\n",
       "      <td>1.287558e+04</td>\n",
       "      <td>6.075241</td>\n",
       "      <td>0.995461</td>\n",
       "      <td>0.924863</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>0.357877</td>\n",
       "      <td>2.018910</td>\n",
       "      <td>12.209123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487980</td>\n",
       "      <td>0.198032</td>\n",
       "      <td>0.478443</td>\n",
       "      <td>0.096005</td>\n",
       "      <td>1.804870</td>\n",
       "      <td>1.678201</td>\n",
       "      <td>3.072731</td>\n",
       "      <td>0.596965</td>\n",
       "      <td>11.803755</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.022228</td>\n",
       "      <td>1.088933e+05</td>\n",
       "      <td>6.886050e+06</td>\n",
       "      <td>0.706851</td>\n",
       "      <td>0.992894</td>\n",
       "      <td>0.945111</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.002335</td>\n",
       "      <td>2.561906</td>\n",
       "      <td>4.082486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296266</td>\n",
       "      <td>0.904720</td>\n",
       "      <td>0.183064</td>\n",
       "      <td>0.232940</td>\n",
       "      <td>1.908443</td>\n",
       "      <td>1.902381</td>\n",
       "      <td>0.388764</td>\n",
       "      <td>0.523826</td>\n",
       "      <td>4.035477</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.339643</td>\n",
       "      <td>1.089560e+05</td>\n",
       "      <td>5.384653e+05</td>\n",
       "      <td>1.219999</td>\n",
       "      <td>0.984684</td>\n",
       "      <td>0.943475</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.009880</td>\n",
       "      <td>2.285058</td>\n",
       "      <td>2.682313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.328416</td>\n",
       "      <td>0.141954</td>\n",
       "      <td>0.325112</td>\n",
       "      <td>0.046463</td>\n",
       "      <td>1.596834</td>\n",
       "      <td>1.708246</td>\n",
       "      <td>0.522859</td>\n",
       "      <td>0.109796</td>\n",
       "      <td>2.630366</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.230005</td>\n",
       "      <td>2.651816e+03</td>\n",
       "      <td>5.384653e+05</td>\n",
       "      <td>0.681214</td>\n",
       "      <td>0.984095</td>\n",
       "      <td>0.943596</td>\n",
       "      <td>0.999702</td>\n",
       "      <td>0.039568</td>\n",
       "      <td>2.292220</td>\n",
       "      <td>2.849284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322286</td>\n",
       "      <td>0.141475</td>\n",
       "      <td>0.319066</td>\n",
       "      <td>0.045444</td>\n",
       "      <td>1.578531</td>\n",
       "      <td>1.704835</td>\n",
       "      <td>0.505560</td>\n",
       "      <td>0.086819</td>\n",
       "      <td>2.805001</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    chi2geo  chi2primfirst  chi2primsecond  chi2topo  cosinefirst  \\\n",
       "0  1.055125   1.191851e+03    2.418227e+03  1.382537     0.992775   \n",
       "1  1.675698   1.191851e+03    2.562767e+00  1.478705     0.992461   \n",
       "2  0.884655   7.708000e+05    6.868500e+03  0.525167     0.986342   \n",
       "3  0.005968   7.835867e+03    6.868500e+03  1.604101     0.986588   \n",
       "4  0.048226   1.085611e+07    9.202689e+05  0.103476     0.928590   \n",
       "5  1.564517   1.755055e+06    3.325792e+06  3.620159     0.986431   \n",
       "6  0.265824   1.272178e+06    1.287558e+04  6.075241     0.995461   \n",
       "7  0.022228   1.088933e+05    6.886050e+06  0.706851     0.992894   \n",
       "8  0.339643   1.089560e+05    5.384653e+05  1.219999     0.984684   \n",
       "9  0.230005   2.651816e+03    5.384653e+05  0.681214     0.984095   \n",
       "\n",
       "   cosinesecond  cosinetopo  distance       eta          l  ...        pT  \\\n",
       "0      0.978738    0.999984  0.010199  1.767213   0.831264  ...  0.858095   \n",
       "1      0.979261    0.995678  0.176995  1.760604   0.462111  ...  0.872880   \n",
       "2      0.971431    0.999988  0.621583  3.087167   6.618644  ...  0.189899   \n",
       "3      0.972632    0.999912  0.122127  3.098664   6.632589  ...  0.185934   \n",
       "4      0.993457    1.000000  0.000261  2.737351   7.693739  ...  0.224500   \n",
       "5      0.917482    0.999995  0.005039  2.155051   6.028908  ...  0.337338   \n",
       "6      0.924863    0.999973  0.357877  2.018910  12.209123  ...  0.487980   \n",
       "7      0.945111    0.999997  0.002335  2.561906   4.082486  ...  0.296266   \n",
       "8      0.943475    0.999985  0.009880  2.285058   2.682313  ...  0.328416   \n",
       "9      0.943596    0.999702  0.039568  2.292220   2.849284  ...  0.322286   \n",
       "\n",
       "        phi        px        py        pz  rapidity         x         y  \\\n",
       "0 -1.419017  0.129742 -0.848230  2.438579  1.629621 -0.138814 -0.281002   \n",
       "1 -1.416307  0.134315 -0.862485  2.463272  1.624846 -0.118300 -0.178443   \n",
       "2 -2.975034 -0.187271 -0.031483  2.076483  2.065397 -0.803517 -0.114543   \n",
       "3 -2.957098 -0.182778 -0.034110  2.056715  2.062181 -0.830276 -0.054652   \n",
       "4  2.629986 -0.195755  0.109911  1.726547  1.875214 -1.039354  0.486576   \n",
       "5  0.300585  0.322212  0.099879  1.435782  1.604478  1.300065  0.455394   \n",
       "6  0.198032  0.478443  0.096005  1.804870  1.678201  3.072731  0.596965   \n",
       "7  0.904720  0.183064  0.232940  1.908443  1.902381  0.388764  0.523826   \n",
       "8  0.141954  0.325112  0.046463  1.596834  1.708246  0.522859  0.109796   \n",
       "9  0.141475  0.319066  0.045444  1.578531  1.704835  0.505560  0.086819   \n",
       "\n",
       "           z  issignal  \n",
       "0   0.784715       1.0  \n",
       "1   0.427863       1.0  \n",
       "2   6.589974       1.0  \n",
       "3   6.602469       1.0  \n",
       "4   7.631403       1.0  \n",
       "5   5.872846       1.0  \n",
       "6  11.803755       1.0  \n",
       "7   4.035477       1.0  \n",
       "8   2.630366       1.0  \n",
       "9   2.805001       1.0  \n",
       "\n",
       "[10 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we remove name prefixes 'Candidates' and do some renaming\n",
    "bckgr.columns = bckgr.columns.str.replace('Candidates_', '')\n",
    "bckgr.columns = bckgr.columns.str.replace('_', '')\n",
    "sign.columns = sign.columns.str.replace('Candidates_', '')\n",
    "sign.columns = sign.columns.str.replace('_', '')\n",
    "#we also get rid of coordinates errors\n",
    "sign = sign.drop(columns=['xerror', 'yerror', 'zerror', 'daughter1id', 'daughter2id', 'pid', 'pTerr', 'etaerr', 'masserr', 'phierr']).rename(columns={'generation' : 'issignal'})\n",
    "bckgr = bckgr.drop(columns=['xerror', 'yerror', 'zerror', 'daughter1id', 'daughter2id', 'pid', 'pTerr', 'etaerr', 'masserr', 'phierr']).rename(columns={'generation' : 'issignal'})\n",
    "#let's check the name prefixes \n",
    "sign.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSmy1dEU72pz"
   },
   "source": [
    "The label 'issignal' tells us if an entry comes from signal (1) or background (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WHuzi6GxB1L"
   },
   "source": [
    "# Data Cleaning\n",
    "Sometimes a data set contains entries which are outliers or does not make sense. For example, infinite values or NaN entries. We clean the data by removing these entries. \n",
    "\n",
    "Similarly, CBM is a fixed target experiment so there are certain conditions which the data has to satisfy before it is considered as reliable data.So we apply certain limits on the data sets.\n",
    "\n",
    "The values of these cuts are described: https://drive.google.com/file/d/1tb0FBRq4KgVu-VQZgpjA8ONbIGVCNOnE/view?usp=sharing https://github.com/julnow/JupyterNotebooks/blob/kaon/CBM%20K-short%20data%20cleaning.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUTS USED FOR DATA CLEANING\n",
    "#mass cuts for both bckgr and sign\n",
    "lowerMassCut = 0.279\n",
    "upperMassCut = 1\n",
    "#distance cuts\n",
    "#DCA\n",
    "lowerDcaCut = 0\n",
    "upperDcaCut = 100\n",
    "#l distance\n",
    "lowerLCut = -5\n",
    "upperLCut = 80\n",
    "#loverdl\n",
    "lowerLdlCut = -25\n",
    "upperLdlCut = 15000\n",
    "#coordinate cuts\n",
    "absXCut = 50\n",
    "absYCut = 50\n",
    "lowerZCut = -1\n",
    "upperZCut = 80\n",
    "#momentums cuts\n",
    "pzLowerCut = 0\n",
    "pUpperCut = 20\n",
    "ptUpperCut = 3\n",
    "#chi2\n",
    "#geo\n",
    "lowerChi2GeoCut = 0\n",
    "upperChi2GeoCut = 10000\n",
    "#topo\n",
    "lowerChi2TopoCut = 0\n",
    "upperChi2TopoCut = 100000\n",
    "#prim first\n",
    "lowerChi2PrimFirstCut = 0\n",
    "upperChi2PrimFirstCut = 3e7\n",
    "#prim second\n",
    "lowerChi2PrimSecondCut = 0\n",
    "upperChi2PrimSecondCut = 3e7\n",
    "#pseudorapidity cuts\n",
    "lowerEtaCut = 1.\n",
    "upperEtaCut = 6.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LEmS1939xEDS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    # let's treat all the infinite, inf, values by nan and then we drop all the null entries\n",
    "    with pd.option_context('mode.use_inf_as_na', True):\n",
    "        df = df.dropna()\n",
    "    #Experimental constraints\n",
    "    is_good_mom = (df['pz'] > pzLowerCut) & (df['p']<pUpperCut) & (df['pT']<ptUpperCut)\n",
    "    is_good_coord = (abs(df['x']) < absXCut) & (abs(df['y']) < absYCut) & (df['z']>lowerZCut) & (df['z']<upperZCut)\n",
    "    is_good_params = (df['distance'] > lowerDcaCut) & (df['distance'] < upperDcaCut) & (df['chi2geo']>lowerChi2GeoCut) & (df['chi2geo'] < upperChi2GeoCut) & (df['chi2topo'] > lowerChi2TopoCut) & (df['chi2topo'] < upperChi2TopoCut) & (df['eta']>lowerEtaCut) & (df['eta']<upperEtaCut)& (df['l']>lowerLCut) & (df['l']<upperLCut) & (df['loverdl']>lowerLdlCut) & (df['loverdl']<upperLdlCut)\n",
    "    is_good_daughters = (df['chi2primfirst']>lowerChi2PrimFirstCut) & (df['chi2primfirst'] < upperChi2PrimSecondCut) & (df['chi2primsecond']>lowerChi2PrimSecondCut) & (df['chi2primsecond']<upperChi2PrimFirstCut)\n",
    "    is_good_mass = (df['mass']>lowerMassCut) & (df['mass']<upperMassCut)\n",
    "\n",
    "    is_good_df = (is_good_mom) & (is_good_coord) & (is_good_params) & (is_good_daughters) & (is_good_mass)\n",
    "\n",
    "    return df[is_good_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "RpWTegAtxF9F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of:\n",
      "signal: 4521056\n",
      "background: 16712900\n",
      "background to signal ratio: 3.7\n"
     ]
    }
   ],
   "source": [
    "background = clean_df(bckgr)\n",
    "signal = clean_df(sign)\n",
    "print('size of:\\nsignal: ' + str(len(signal)) + '\\nbackground: ' + str(len(background)) \n",
    "      + '\\nbackground to signal ratio: ' + str(round(len(background)/len(signal), 1)))\n",
    "del sign, bckgr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also import and clean dataset of 10k events for both background and signal (for testing our algorhitm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OUTTkp2072qI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's combine signal and background\n",
    "df_scaled = pd.concat([signal, background])\n",
    "del signal, background\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4Ie_PTM72qH"
   },
   "source": [
    "# Selecting Background and Signal\n",
    "After cleaning, our training data set contains 450442 background candidates and 1638402 signal candidates (background to signal ratio $\\approx 3.6$). For testing, we'll use cleaned dataset containing 28740227 entries of signal and background (background to signal ratio $\\approx 1985.1$)\n",
    "\n",
    "Here, we use under-sampling to tackle the problem with the classification of underrepresented class (normally, the signal is only approx. 0.05% of real data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(directory+'img/xgb_12agev/urqmd'):\n",
    "    os.makedirs(directory+'img/xgb_12agev/urqmd')\n",
    "#lets draw sign to background ratio\n",
    "def plt_sig_back(df):\n",
    "    range1 = (lowerMassCut, upperMassCut)\n",
    "    fig, axs = plt.subplots(figsize=(20, 10))\n",
    "    #df_scaled['mass'].plot.hist(bins = 300, range=range1,grid=True,sharey=True)\n",
    "    (df[df['issignal']==0])['mass'].plot.hist(bins = 300, facecolor='yellow',grid=True,range=range1, label='Background')\n",
    "    (df[df['issignal']==1])['mass'].plot.hist(bins = 300, facecolor='magenta',grid=True, range=range1, label ='Signal')\n",
    "    #plt.vlines(x=1.108,ymin=-1,ymax=48000, color='black', linestyle='-')\n",
    "    #plt.vlines(x=1.1227,ymin=-1,ymax=48000, color='black', linestyle='-')\n",
    "    plt.ylabel(\"Counts (log scale)\", fontsize=15)\n",
    "    plt.xlabel(\"Mass in GeV/$c^2$\", fontsize= 15)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.title('Test and Train K-short Invariant Mass', fontsize = 15)\n",
    "    plt.legend( fontsize = 15)\n",
    "    axs.tick_params(axis='both', which='major', labelsize=18)\n",
    "    plt.yscale(\"log\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(directory+'img/xgb_12agev/urqmd/inv_mass_trainset.pdf')\n",
    "    fig.savefig(directory+'img/xgb_12agev/urqmd/inv_mass_trainset.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AVQErryc72qc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1425"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import matplotlib.font_manage\n",
    "#plt.get_cache_dir()\n",
    "plt_sig_back(df_scaled)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyveX9Lh72qk"
   },
   "source": [
    "# Creating Train and Test sets\n",
    "To make machine learning algorithms more efficient on unseen data we divide our data into two sets. One set is for training the algorithm and the other is for testing the algorithm. If we don't do this then the algorithm can overfit and we will not capture the general trends in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VOEQTRhb72ql"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44638"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following columns will be used to predict whether a reconstructed candidate is a lambda particle or not\n",
    "cuts = [ 'loverdl', 'distance', 'chi2topo', 'chi2primfirst', 'chi2primsecond']#'chi2geo', \n",
    "\n",
    "\n",
    "x = df_scaled[cuts].copy()\n",
    "\n",
    "# The MC information is saved in this y variable\n",
    "y =pd.DataFrame(df_scaled['issignal'], dtype='int')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=324)\n",
    "del df_scaled, x, y\n",
    "\n",
    "#DMatrix is a internal data structure that used by XGBoost which is optimized for both memory efficiency and training speed. \n",
    "dtrain = xgb.DMatrix(x_train, label = y_train)\n",
    "dtest1=xgb.DMatrix(x_test, label = y_test)\n",
    "del x_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FihmBwJ72rE"
   },
   "source": [
    "## Bayesian optimization\n",
    "In order to find the best parameters of XGB for our data we use Bayesian optimization. Grid search and and random search could also do the same job but bayesian is more time efficient. For further reading visit [the git page](https://github.com/fmfn/BayesianOptimization) of the bayesian optimization used here.\n",
    "\n",
    "### Hyper parameters\n",
    "Some of the following hyper parameters will be tuned for our algorithm:\n",
    "\n",
    "\n",
    "*subsample* [default=1]\n",
    "Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.\n",
    "range: (0,1]\n",
    "\n",
    "*eta* [default=0.3, alias: learning_rate]\n",
    "Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n",
    "range: [0,1]\n",
    "\n",
    "\n",
    "*gamma* [default=0, alias: min_split_loss]\n",
    "Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n",
    "range: [0,âˆž]\n",
    "\n",
    "\n",
    "*alpha* [default=0, alias: reg_alpha]\n",
    "L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "\n",
    "*Lasso Regression* (Least Absolute Shrinkage and Selection Operator) adds â€œabsolute value of magnitudeâ€ of coefficient as penalty term to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wL9CUKGT72rc"
   },
   "outputs": [],
   "source": [
    "#Bayesian Optimization function for xgboost\n",
    "#specify the parameters you want to tune as keyword arguments\n",
    "def bo_tune_xgb(max_depth, gamma, alpha, n_estimators ,learning_rate):\n",
    "    params = {'max_depth': int(max_depth),\n",
    "              'gamma': gamma,\n",
    "              'alpha':alpha,\n",
    "              'n_estimators': n_estimators,\n",
    "              'learning_rate':learning_rate,\n",
    "              'subsample': 0.8,\n",
    "              'eta': 0.1,\n",
    "              'eval_metric': 'auc', 'nthread' : 8}\n",
    "    cv_result = xgb.cv(params, dtrain, num_boost_round=10, nfold=5)\n",
    "    return  cv_result['test-auc-mean'].iloc[-1]\n",
    "\n",
    "bounds_transformer = SequentialDomainReductionTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "eN20_fJV72rg"
   },
   "outputs": [],
   "source": [
    "#Invoking the Bayesian Optimizer with the specified parameters to tune\n",
    "xgb_bo = BayesianOptimization(bo_tune_xgb, {'max_depth': (6, 16),\n",
    "                                             'gamma': (0, 1),\n",
    "                                            'alpha': (2,12),\n",
    "                                             'learning_rate':(0,.5),\n",
    "                                             'n_estimators':(300,1000)\n",
    "                                            },\n",
    "                             bounds_transformer=bounds_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "OW00bW2i72rn",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7bc098cd-98e6-4e2a-a689-75b55a4dc004",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   |   gamma   | learni... | max_depth | n_esti... |\n",
      "-------------------------------------------------------------------------------------\n",
      "[01:44:40] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:44:58] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:45:16] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:45:34] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:45:51] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.9974  \u001b[0m | \u001b[0m 2.637   \u001b[0m | \u001b[0m 0.7059  \u001b[0m | \u001b[0m 0.342   \u001b[0m | \u001b[0m 9.514   \u001b[0m | \u001b[0m 536.9   \u001b[0m |\n",
      "[02:00:17] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:00:33] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:00:50] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:01:06] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:01:23] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.9972  \u001b[0m | \u001b[0m 8.681   \u001b[0m | \u001b[0m 0.2278  \u001b[0m | \u001b[0m 0.2134  \u001b[0m | \u001b[0m 9.336   \u001b[0m | \u001b[0m 815.6   \u001b[0m |\n",
      "[02:15:05] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:15:18] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:15:32] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:15:45] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:15:59] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.9971  \u001b[0m | \u001b[0m 3.079   \u001b[0m | \u001b[0m 0.4716  \u001b[0m | \u001b[0m 0.1259  \u001b[0m | \u001b[0m 7.132   \u001b[0m | \u001b[0m 606.3   \u001b[0m |\n",
      "[02:27:29] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:27:47] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:28:05] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:28:23] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:28:40] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.997   \u001b[0m | \u001b[0m 4.07    \u001b[0m | \u001b[0m 0.1242  \u001b[0m | \u001b[0m 0.04073 \u001b[0m | \u001b[0m 10.48   \u001b[0m | \u001b[0m 343.9   \u001b[0m |\n",
      "[02:43:14] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:43:30] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:43:47] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:44:04] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:44:20] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.9972  \u001b[0m | \u001b[0m 3.561   \u001b[0m | \u001b[0m 0.9867  \u001b[0m | \u001b[0m 0.1518  \u001b[0m | \u001b[0m 9.829   \u001b[0m | \u001b[0m 630.7   \u001b[0m |\n",
      "[02:58:09] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:58:27] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:58:45] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:59:03] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:59:20] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 0.9974  \u001b[0m | \u001b[95m 2.346   \u001b[0m | \u001b[95m 0.5516  \u001b[0m | \u001b[95m 0.2948  \u001b[0m | \u001b[95m 10.47   \u001b[0m | \u001b[95m 537.2   \u001b[0m |\n",
      "[03:14:18] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:14:38] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:14:58] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:15:19] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:15:38] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.9974  \u001b[0m | \u001b[95m 3.843   \u001b[0m | \u001b[95m 0.5084  \u001b[0m | \u001b[95m 0.3211  \u001b[0m | \u001b[95m 12.68   \u001b[0m | \u001b[95m 539.6   \u001b[0m |\n",
      "[03:32:39] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:33:02] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:33:25] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:33:49] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:34:13] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.9975  \u001b[0m | \u001b[95m 2.15    \u001b[0m | \u001b[95m 0.5851  \u001b[0m | \u001b[95m 0.3065  \u001b[0m | \u001b[95m 14.56   \u001b[0m | \u001b[95m 535.3   \u001b[0m |\n",
      "[03:53:54] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:54:20] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:54:46] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:55:10] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:55:34] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.9975  \u001b[0m | \u001b[95m 2.123   \u001b[0m | \u001b[95m 0.5502  \u001b[0m | \u001b[95m 0.3246  \u001b[0m | \u001b[95m 15.31   \u001b[0m | \u001b[95m 547.3   \u001b[0m |\n",
      "[04:16:07] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:16:29] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:16:52] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:17:15] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:17:37] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.9975  \u001b[0m | \u001b[0m 2.142   \u001b[0m | \u001b[0m 0.4129  \u001b[0m | \u001b[0m 0.3163  \u001b[0m | \u001b[0m 13.44   \u001b[0m | \u001b[0m 555.8   \u001b[0m |\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "#performing Bayesian optimization for 5 iterations with 8 steps of random exploration with an #acquisition function of expected improvement\n",
    "xgb_bo.maximize(n_iter=5, init_points=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ebJXjFdP72rv",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target': 0.9974869999999999, 'params': {'alpha': 2.1229255510432457, 'gamma': 0.5501634869520728, 'learning_rate': 0.3246106856705202, 'max_depth': 15.310923531429589, 'n_estimators': 547.3434520185153}}\n"
     ]
    }
   ],
   "source": [
    "# best target so far 0.9974344\n",
    "print(xgb_bo.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we got: {'target': 0.997335, 'params': {'alpha': 5.753533575127802, 'gamma': 0.9381973111303026, 'learning_rate': 0.2632517674042119, 'max_depth': 15.56604307847495, 'n_estimators': 881.7816101314299}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOauOey772r3",
    "tags": []
   },
   "source": [
    "# XGB models\n",
    "Now let's take the parameters selected by the bayesian optimization and apply them in our training and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "j8eiJzKwpwXJ"
   },
   "outputs": [],
   "source": [
    "max_params = {'target': 0.9974869999999999, 'params': {'alpha': 2.1229255510432457, 'gamma': 0.5501634869520728, 'learning_rate': 0.3246106856705202, 'max_depth': 15.310923531429589, 'n_estimators': 547.3434520185153}}\n",
    "#max_params = {'target': 0.997335, 'params': {'alpha': 5.753533575127802, 'gamma': 0.9381973111303026, 'learning_rate': 0.2632517674042119, 'max_depth': 15.56604307847495, 'n_estimators': 881.7816101314299}}\n",
    "max_param = max_params['params']#xgb_bo.max['params']\n",
    "param= {'alpha': max_param['alpha'], 'gamma': max_param['gamma'], 'learning_rate': max_param['learning_rate'],\n",
    "        'max_depth': int(round(max_param['max_depth'],0)), 'n_estimators': int(round(max_param['n_estimators'],0))\n",
    "        , 'objective': 'binary:logistic','nthread' : 8}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OiTX1iSx72r9",
    "outputId": "01ab4a24-aa9e-4f00-8322-31f0f63ac224"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:35:27] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:36:10] WARNING: /Users/ktietz/demo/mc3/conda-bld/xgboost-split_1628682908089/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "#To train the algorithm using the parameters selected by bayesian optimization\n",
    "bst = xgb.train(param, dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "G6U8lnP1jJzI"
   },
   "outputs": [],
   "source": [
    "#We apply our model on the training data that was trained on the training data, this helps us to control overfitting\n",
    "bst1= bst.predict(dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bst = xgb.XGBClassifier(**param).fit(x_train, y_train) #this is the right way to use it but takes a lot of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bst1= bst.predict(dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del dtrain \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "m2p8YPY1jRQ_"
   },
   "outputs": [],
   "source": [
    "# We apply our trained model on test data and store the predictions in a bst_test dataframe\n",
    "bst_test = pd.DataFrame(data=bst.predict(dtest1),  columns=[\"xgb_preds\"])\n",
    "y_test=y_test.set_index(np.arange(0,bst_test.shape[0]))\n",
    "# We also store the MC information in this dataFrame\n",
    "bst_test['issignal']=y_test['issignal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole set\n",
    "We also select the selected variables from the 10k events data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set (background to signal ratio): 1858.5\n",
      "test set (both signal and background): 55509831\n"
     ]
    }
   ],
   "source": [
    "df_origin = tree_importer.tree_importer(allURQMD,'PlainTree',7)\n",
    "df_origin.columns = df_origin.columns.str.replace('Candidates_', '')\n",
    "df_origin.columns = df_origin.columns.str.replace('_', '')\n",
    "df_origin = df_origin.drop(columns=['xerror', 'yerror', 'zerror', 'daughter1id', 'daughter2id', 'pid', 'pTerr', 'etaerr', 'masserr', 'phierr']).rename(columns={'generation' : 'issignal'})\n",
    "df_clean = clean_df(df_origin)\n",
    "del df_origin\n",
    "gc.collect()\n",
    "#lets look at impoted tree\n",
    "df_clean.iloc[0:10,:]\n",
    "print('test set (background to signal ratio): ' \n",
    "      + str(round(len(df_clean.loc[df_clean['issignal'] == 0])/len(df_clean.loc[df_clean['issignal'] == 1]), 1))\n",
    "      + '\\ntest set (both signal and background): ' + str(len(df_clean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_whole = df_clean[cuts].copy()\n",
    "y_whole = pd.DataFrame(df_clean['issignal'], dtype='int')\n",
    "dtest = xgb.DMatrix(x_whole, label = y_whole)\n",
    "del x_whole\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "5Xm-sY3W72s9",
    "outputId": "878e260c-66bc-4571-f7ef-33a5a7084e09"
   },
   "outputs": [],
   "source": [
    "#The following graph will show us that which features are important for the model\n",
    "ax = xgb.plot_importance(bst)\n",
    "plt.rcParams['figure.figsize'] = [5, 3]\n",
    "plt.show()\n",
    "ax.figure.tight_layout() \n",
    "ax.figure.savefig(directory+'img/xgb_12agev/urqmd/feature_importance.pdf')\n",
    "ax.figure.savefig(directory+'img/xgb_12agev/urqmd/feature_importance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "To2uJSUWAESb"
   },
   "source": [
    "## AUC and ROC\n",
    "\n",
    "The function roc_curve computes the receiver operating characteristic curve, or ROC curve. Quoting Wikipedia :\n",
    "\n",
    "â€œA receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate.â€\n",
    "\n",
    "This function requires the true binary value and the target scores, which can either be probability estimates of the positive class, confidence values, or binary decisions.\n",
    "\n",
    "Similarly, the function roc_auc_score computes Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YV_FtX0MUVj-"
   },
   "source": [
    "To find the best threshold which results more signal to background ratio for lambda candidates we use the parameter S0 called the approximate median significance by the higgs boson  ML challenge (http://higgsml.lal.in2p3.fr/documentation,9.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-jWtHorkGQ9"
   },
   "outputs": [],
   "source": [
    "def AMS(y_true, y_predict, y_true1, y_predict1):\n",
    "    roc_auc=roc_auc_score(y_true, y_predict)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_predict,drop_intermediate=False ,pos_label=1)\n",
    "    S0 = sqrt(2 * ((tpr + fpr) * log((1 + tpr/fpr)) - tpr))\n",
    "    S0 = S0[~np.isnan(S0)]\n",
    "    xi = argmax(S0)\n",
    "    S0_best_threshold = (thresholds[xi])\n",
    "\n",
    "    roc_auc1=roc_auc_score(y_true1, y_predict1)\n",
    "    fpr1, tpr1, thresholds1 = roc_curve(y_true1, y_predict1,drop_intermediate=False ,pos_label=1)\n",
    "    S01 = sqrt(2 * ((tpr1 + fpr1) * log((1 + tpr1/fpr1)) - tpr1))\n",
    "    S01 = S01[~np.isnan(S01)]\n",
    "    xi1 = argmax(S01)\n",
    "    S0_best_threshold1 = (thresholds[xi1])\n",
    "\n",
    "    #plotting\n",
    "    fig, axs = plt.subplots(figsize=(15, 10), dpi = 100)\n",
    "    plt.plot(fpr, tpr, linestyle=':',color='darkorange',label='ROC curve train (area = %0.4f)' % roc_auc)\n",
    "    plt.plot(fpr1, tpr1, color='green',label='ROC curve test (area = %0.4f)' % roc_auc1)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.scatter(fpr[xi], tpr[xi], marker='o', color='black', label= 'Best Threshold train set = '+\"%.4f\" % S0_best_threshold +'\\n S0 = '+ \"%.2f\" % S0[xi])\n",
    "    plt.scatter(fpr1[xi1], tpr1[xi1], marker='o', color='blue', label= 'Best Threshold test set = '+\"%.4f\" % S0_best_threshold1 +'\\n S0 = '+ \"%.2f\" % S01[xi1])\n",
    "    plt.xlabel('False Positive Rate', fontsize = 15)\n",
    "    plt.ylabel('True Positive Rate', fontsize = 15)\n",
    "    plt.legend(loc=\"lower right\", fontsize = 15)\n",
    "    plt.title('Receiver operating characteristic', fontsize = 15)\n",
    "    plt.xlim([-0.01, 1.0])\n",
    "    plt.ylim([0, 1.02])\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig(directory+'img/xgb_12agev/urqmd/ams.pdf')\n",
    "    fig.savefig(directory+'img/xgb_12agev/urqmd/ams.png')\n",
    "\n",
    "    return S0_best_threshold, S0_best_threshold1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 933
    },
    "id": "JvOr9TwvkIZe",
    "outputId": "e70284c8-182b-4a9f-bc40-660372557382"
   },
   "outputs": [],
   "source": [
    "train_best, test_best = AMS(y_train, bst1, y_test, bst_test['xgb_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preds_prob(df, preds, true, dataset):\n",
    "    if dataset =='train':\n",
    "        label1 = 'XGB Predictions on the training data set'\n",
    "    else:\n",
    "        label1 = 'XGB Predictions on the test data set'\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    bins1=100\n",
    "    plt.hist(df[preds], bins=bins1,facecolor='green',alpha = 0.3, label=label1)\n",
    "    TP = df[(df[true]==1)]\n",
    "    TN = df[(df[true]==0)]\n",
    "    #TP[preds].plot.hist(ax=ax, bins=bins1,facecolor='blue', histtype='stepfilled',alpha = 0.3, label='True Positives/signal in predictions')\n",
    "    hist, bins = np.histogram(TP[preds], bins=bins1)\n",
    "    err = np.sqrt(hist)\n",
    "    center = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "    \n",
    "    hist1, bins1 = np.histogram(TN[preds], bins=bins1)\n",
    "    err1 = np.sqrt(hist1)\n",
    "    plt.errorbar(center, hist1, yerr=err1, fmt='o',\n",
    "                 c='Red', label='Background in predictions')\n",
    "    \n",
    "    plt.errorbar(center, hist, yerr=err, fmt='o',\n",
    "                 c='blue', label='Signal in predictions')\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "    plt.xlabel('Probability',fontsize=18)\n",
    "    plt.ylabel('Counts', fontsize=18)\n",
    "    plt.legend(fontsize=18)\n",
    "    ax.set_xticks(np.arange(0,1.1,0.1))\n",
    "    ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=16)\n",
    "    plt.show()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig('test_best.png')\n",
    "preds_prob(bst_test,'xgb_preds', 'issignal','test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOjnbDBbpB8r"
   },
   "source": [
    "When the AUC, best threshold and approximate median significance for train and test are nearly the same, we save that model and use it. This means that our model is general enough. In my opinion, if the test S0 is above 3.0 then it is a good enough model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the model on the 10k events data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['xgb_preds'] = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['xgb_preds'].hist(bins=300)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loqnFETE_1W1"
   },
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A **Confusion Matrix** $C$ is such that $C_{ij}$ is equal to the number of observations known to be in group $i$ and predicted to be in group $j$. Thus in binary classification, the count of true positives is $C_{00}$, false negatives $C_{01}$,false positives is $C_{10}$, and true neagtives is $C_{11}$.\n",
    "\n",
    "If $ y^{'}_{i} $ is the predicted value of the $ i$-th sample and $y_{i}$ is the corresponding true value, then the fraction of correct predictions over $ n_{samples}$ is defined as \n",
    "$$\n",
    "True \\: positives (y,y^{'}) =  \\sum_{i=1}^{n_{samples} } 1 (y^{'}_{i} = y_{i}=1)\n",
    "$$ \n",
    "\n",
    "The following function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQrDUlyr_07i"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',fontsize = 15)\n",
    "    plt.xlabel('Predicted label',fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate efficency and false tu tru signal ratio\n",
    "def confustion_stats(df, cm):\n",
    "    all_signals = len(df.loc[df['issignal'] == 1])\n",
    "    true_signal = cm[0][0]\n",
    "    false_signal = cm[1][0]        \n",
    "    reconstructed_signals = true_signal + false_signal\n",
    "    false_to_true_signals = false_signal / true_signal\n",
    "    efficiency = reconstructed_signals / all_signals * 100 #efficency in % for all\n",
    "    efficiency_true = true_signal / all_signals * 100 #efficency in % for all\n",
    "    print(\"Efficiency: \" + str(round(efficiency, 2)) + \"%\")\n",
    "    print(\"Efficiency of true signal candidates reconstruction: \" + str(round(efficiency_true, 2)) + \"%\")\n",
    "    print(\"False tu true reconstructed signal ratio: \" + str(round(false_to_true_signals, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_best=0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 636
    },
    "id": "QP6WiKgQ_8yG",
    "outputId": "83f8a707-7b05-4ab5-e50f-e9ef69c2cfcd"
   },
   "outputs": [],
   "source": [
    "#lets take the best threshold and look at the confusion matrix\n",
    "cut1 = test_best\n",
    "df_clean['xgb_preds1'] = ((df_clean['xgb_preds']>cut1)*1) #1==signal decided by probability\n",
    "cnf_matrix = confusion_matrix(y_whole, df_clean['xgb_preds1'], labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "fig, axs = plt.subplots(figsize=(10, 8))\n",
    "axs.yaxis.set_label_coords(-0.04,.5)\n",
    "axs.xaxis.set_label_coords(0.5,-.005)\n",
    "plot_confusion_matrix(cnf_matrix, classes=['signal','background'], title='Confusion Matrix for XGB for cut > '+str(cut1))\n",
    "fig.savefig(directory+'img/xgb_12agev/urqmd/confusion_matrix_extreme_gradient_boosting_whole_data.pdf')\n",
    "fig.savefig(directory+'img/xgb_12agev/urqmd/confusion_matrix_extreme_gradient_boosting_whole_data.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confustion_stats(df_clean, cnf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with old params from gradient boosting, but new xgb\n",
    "Efficiency: 88.1%\n",
    "Efficiency of true signal candidates reconstruction: 85.3%\n",
    "False tu true reconstructed signal ratio: 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtZSmZ8kAZg5"
   },
   "outputs": [],
   "source": [
    "# The following function will display the inavriant mass histogram of the original 10k event set along with the mass histoigram after we apply a cut\n",
    "# on the probability prediction of xgb\n",
    "def cut_visualization(cut, range1=(lowerMassCut, upperMassCut), bins1= 300 ):\n",
    "    mask1 = df_clean['xgb_preds']>cut\n",
    "    df3=df_clean[mask1]\n",
    "    \n",
    "    fig, ax2 = plt.subplots(figsize=(15, 10), dpi = 200)\n",
    "    color = 'tab:blue'\n",
    "    ax2.hist(df_clean['mass'],bins = bins1, range=range1, facecolor='blue',alpha = 0.35, label='before selection')\n",
    "    ax2.set_ylabel('Counts', fontsize = 15, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.legend( fontsize = 15, loc='upper left')\n",
    "    \n",
    "    color = 'tab:red'\n",
    "    ax1 = ax2.twinx()\n",
    "    ax1.hist(df3['mass'], bins = bins1, range=range1, facecolor='red',alpha = 0.35, label='XGB')\n",
    "    ax1.set_xlabel('Mass in GeV', fontsize = 15)\n",
    "    ax1.set_ylabel('Counts ', fontsize = 15, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.legend( fontsize = 15,loc='upper right' )\n",
    "\n",
    "    plt.title(\"The original sample's Invariant Mass along with mass after selection of XGB (with a cut > \"+str(cut)+')', fontsize = 15)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(directory+'img/xgb_12agev/urqmd/whole_sample_invmass_with_ML.pdf')\n",
    "    fig.savefig(directory+'img/xgb_12agev/urqmd/whole_sample_invmass_with_ML.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JOnsE6BTAfdx",
    "outputId": "f480f13b-c2ce-44c8-a639-cd849766ce80"
   },
   "outputs": [],
   "source": [
    "cut_visualization(test_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrItzy4C72uS"
   },
   "source": [
    "# Comparison with the manually optimized cuts of KFPF\n",
    "In the already existing Kalman Filter Particle Finder (KFPF) package for online reconstruction and selection of short-lived particles in CBM, these criteria have been manually optimized. These selection-cuts have been selected to maximize the signal to background ratio (S/B) of the $\\Lambda$ for a certain energy on a collisions generator. The selection criteria mainly depends on the collision energy, decay channel and detector configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually selected cuts\n",
    "manCut_loverdl = 30\n",
    "manCut_dca = 0.4\n",
    "manCut_chi2topo = 20\n",
    "manCut_chi2geo = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQrDUlyr_07i"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',fontsize = 15)\n",
    "    plt.xlabel('Predicted label',fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 636
    },
    "id": "A0CXPHGR72uS",
    "outputId": "c94f8fd1-0499-4f48-d284-4c639b5f7462"
   },
   "outputs": [],
   "source": [
    "new_check_set= df_clean.copy()\n",
    "new_check_set['new_signal']=0\n",
    "\n",
    "mask1 = (new_check_set['loverdl'] > manCut_loverdl) & (new_check_set['distance'] < manCut_dca)\n",
    "\n",
    "mask2 = (new_check_set['chi2topo'] < manCut_chi2topo) & (new_check_set['chi2geo'] < manCut_chi2geo) \n",
    "\n",
    "new_check_set = new_check_set[(mask1) & (mask2)]\n",
    "\n",
    "# antimask1 = !(new_check_set['loverdl'] > manCut_loverdl) & (new_check_set['distance'] < manCut_dca)\n",
    "\n",
    "# antimask2 = !(new_check_set['chi2topo'] < manCut_chi2topo) & (new_check_set['chi2geo'] < manCut_chi2geo) \n",
    "\n",
    "# new_anticheck_set = new_check_set[(antimask1) & (antimask2)]\n",
    "\n",
    "#After all these cuts, what is left is considered as signal, so we replace all the values in the 'new_signal'\n",
    "# column by 1\n",
    "new_check_set['new_signal'] = 1\n",
    "# new_anticheck_set['new_signal'] = 0\n",
    "cnf_matrix1 = confusion_matrix(new_check_set['issignal'], new_check_set['new_signal'], labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "fig, axs = plt.subplots(figsize=(10, 8))\n",
    "axs.yaxis.set_label_coords(-0.04,.5)\n",
    "axs.xaxis.set_label_coords(0.5,-.005)\n",
    "plot_confusion_matrix(cnf_matrix1, classes=['signal','background'], title='Confusion Matrix for manually set cuts')\n",
    "fig.savefig(directory+'img/xgb_12agev/urqmd/confusion_matrix_for_manually_set_cuts.pdf')\n",
    "fig.savefig(directory+'img/xgb_12agev/urqmd/confusion_matrix_for_manually_set_cuts.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confustion_stats(df_clean, cnf_matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ncr9MQSdD7WD"
   },
   "outputs": [],
   "source": [
    "cut3 = test_best\n",
    "mask1 = df_clean['xgb_preds']>cut3\n",
    "df3=df_clean[mask1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 654
    },
    "id": "DwAD5im6lIj4",
    "outputId": "3660034f-8f9b-472b-aced-35746f9ad1ee"
   },
   "outputs": [],
   "source": [
    "# from matplotlib import gridspec\n",
    "\n",
    "range1= (lowerMassCut, upperMassCut)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 1,figsize=(15,10), sharex=True,  gridspec_kw={'width_ratios': [10],\n",
    "                           'height_ratios': [8,4]})\n",
    "\n",
    "ns, bins, patches=axs[0].hist((df3['mass']),bins = 300, range=range1, facecolor='red',alpha = 0.3)\n",
    "ns1, bins1, patches1=axs[0].hist((new_check_set['mass']),bins = 300, range=range1,facecolor='blue',alpha = 0.3)\n",
    "#plt.xlabel(\"Mass in GeV\", fontsize = 15)\n",
    "axs[0].set_ylabel(\"counts\", fontsize = 15)\n",
    "#axs[0].grid()\n",
    "axs[0].legend(('XGBoost Selected K-short','KFPF selected K-short'), fontsize = 15, loc='upper right')\n",
    "\n",
    "#plt.rcParams[\"legend.loc\"] = 'upper right'\n",
    "axs[0].set_title(\"The K-short Invariant Mass histogram with KFPF and XGB selection criteria on KFPF variables\", fontsize = 15)\n",
    "axs[0].grid()\n",
    "axs[0].tick_params(axis='both', which='major', labelsize=15)\n",
    "#fig.savefig(\"whole_sample_invmass_with_ML.png\")\n",
    "\n",
    "\n",
    "hist1, bin_edges1 = np.histogram(df3['mass'],range=(lowerMassCut, upperMassCut), bins=200)\n",
    "hist2, bin_edges2 = np.histogram(new_check_set['mass'],range=(lowerMassCut, upperMassCut), bins=200)\n",
    "\n",
    "#ratio checked with division by 0\n",
    "ratioBlue = [] #ratio smaller than 1\n",
    "ratioRed = [] # ratio bigger than 1\n",
    "for i in range(len(ns1)):\n",
    "    if (ns1[i] != 0):\n",
    "        ratio = ns[i] / ns1[i]\n",
    "        if (ratio < 1):\n",
    "            ratioBlue.append(ratio)\n",
    "            ratioRed.append(0)\n",
    "        else:\n",
    "            ratioBlue.append(0)\n",
    "            ratioRed.append(ratio)\n",
    "    else:\n",
    "        ratioBlue.append(0)\n",
    "        ratioRed.append(0)\n",
    "#col = []\n",
    "# for val in ratio:\n",
    "#     if val < 1:\n",
    "#         col.append('blue')\n",
    "#     else:\n",
    "#         col.append('red')        \n",
    "axs[1].bar(bins[:-1],     \n",
    "        ratioBlue, \n",
    "        width=0.001,\n",
    "        color = 'blue',\n",
    "        label='ratio < 1')\n",
    "axs[1].bar(bins[:-1],     \n",
    "        ratioRed, \n",
    "        width=0.001,\n",
    "        color = 'red',\n",
    "        label='ratio >= 1')\n",
    "axs[1].legend( loc='upper right')\n",
    "plt.xlabel(\"Mass in $\\dfrac{GeV}{c^2}$\", fontsize = 15)\n",
    "axs[1].set_ylabel(\"XGB / KFPF\", fontsize = 15)\n",
    "axs[1].grid()\n",
    "#axs[1].set_ylim([0.,2.5])\n",
    "axs[1].tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "plt.show()\n",
    "fig.tight_layout()\n",
    "fig.savefig(directory+'img/xgb_12agev/urqmd/kaon_inv_mass_comparison.png')\n",
    "fig.savefig(directory+'img/xgb_12agev/urqmd/kaon_inv_mass_comparison.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 654
    },
    "id": "DwAD5im6lIj4",
    "outputId": "3660034f-8f9b-472b-aced-35746f9ad1ee"
   },
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "range1= (0.4, 0.6)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 1,figsize=(15,10), sharex=True,  gridspec_kw={'width_ratios': [10],\n",
    "                           'height_ratios': [8,4]})\n",
    "\n",
    "ns, bins, patches=axs[0].hist((df3['mass']),bins = 100, range=range1, facecolor='red',alpha = 0.3)\n",
    "ns1, bins1, patches1=axs[0].hist((new_check_set['mass']),bins = 100, range=range1,facecolor='blue',alpha = 0.3)\n",
    "#plt.xlabel(\"Mass in GeV\", fontsize = 15)\n",
    "axs[0].set_ylabel(\"counts\", fontsize = 15)\n",
    "#axs[0].grid()\n",
    "axs[0].legend(('XGBoost Selected K-short','KFPF selected K-short'), fontsize = 15, loc='upper right')\n",
    "\n",
    "#plt.rcParams[\"legend.loc\"] = 'upper right'\n",
    "axs[0].set_title(\"The K-short Invariant Mass histogram with KFPF and XGB selection criteria on KFPF variables (close up)\", fontsize = 15)\n",
    "axs[0].grid()\n",
    "axs[0].tick_params(axis='both', which='major', labelsize=15)\n",
    "#fig.savefig(\"whole_sample_invmass_with_ML.png\")\n",
    "\n",
    "\n",
    "hist1, bin_edges1 = np.histogram(df3['mass'],range=(lowerMassCut, upperMassCut), bins=100)\n",
    "hist2, bin_edges2 = np.histogram(new_check_set['mass'],range=(lowerMassCut, upperMassCut), bins=100)\n",
    "\n",
    "#ratio checked with division by 0\n",
    "ratioBlue = [] #ratio smaller than 1\n",
    "ratioRed = [] # ratio bigger than 1\n",
    "for i in range(len(ns1)):\n",
    "    if (ns1[i] != 0):\n",
    "        ratio = ns[i] / ns1[i]\n",
    "        if (ratio < 1):\n",
    "            ratioBlue.append(ratio)\n",
    "            ratioRed.append(0)\n",
    "        else:\n",
    "            ratioBlue.append(0)\n",
    "            ratioRed.append(ratio)\n",
    "    else:\n",
    "        ratioBlue.append(0)\n",
    "        ratioRed.append(0)\n",
    "#col = []\n",
    "# for val in ratio:\n",
    "#     if val < 1:\n",
    "#         col.append('blue')\n",
    "#     else:\n",
    "#         col.append('red')        \n",
    "axs[1].bar(bins[:-1],     \n",
    "        ratioBlue, \n",
    "        width=0.001,\n",
    "        color = 'blue',\n",
    "        label='ratio < 1')\n",
    "axs[1].bar(bins[:-1],     \n",
    "        ratioRed, \n",
    "        width=0.001,\n",
    "        color = 'red',\n",
    "        label='ratio >= 1')\n",
    "axs[1].legend( loc='upper right')\n",
    "plt.xlabel(\"Mass in $\\dfrac{GeV}{c^2}$\", fontsize = 15)\n",
    "axs[1].set_ylabel(\"XGB / KFPF\", fontsize = 15)\n",
    "axs[1].grid()\n",
    "axs[1].set_ylim([0.,2.5])\n",
    "axs[1].tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "plt.show()\n",
    "fig.tight_layout()\n",
    "fig.savefig(directory+'img/xgb_12agev/urqmd/kaon_inv_mass_comparison_closeup.png')\n",
    "fig.savefig(directory+'img/xgb_12agev/urqmd/kaon_inv_mass_comparison_closeup.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 654
    },
    "id": "DwAD5im6lIj4",
    "outputId": "3660034f-8f9b-472b-aced-35746f9ad1ee"
   },
   "outputs": [],
   "source": [
    "# from matplotlib import gridspec\n",
    "\n",
    "range1= (lowerMassCut, upperMassCut)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 1,figsize=(15,10), sharex=True,  gridspec_kw={'width_ratios': [10],\n",
    "                           'height_ratios': [8,4]})\n",
    "\n",
    "ns, bins, patches=axs[0].hist((df3['mass']),bins = 300, range=range1, facecolor='red',alpha = 0.3)\n",
    "ns1, bins1, patches1=axs[0].hist((new_check_set['mass']),bins = 300, range=range1,facecolor='blue',alpha = 0.3)\n",
    "#plt.xlabel(\"Mass in GeV\", fontsize = 15)\n",
    "axs[0].set_ylabel(\"counts\", fontsize = 15)\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].legend(('XGBoost Selected K-short','KFPF selected K-short'), fontsize = 15, loc='upper right')\n",
    "\n",
    "#plt.rcParams[\"legend.loc\"] = 'upper right'\n",
    "axs[0].set_title(\"The K-short Invariant Mass histogram with KFPF and XGB selection criteria on KFPF variables\", fontsize = 15)\n",
    "axs[0].grid()\n",
    "axs[0].tick_params(axis='both', which='major', labelsize=15)\n",
    "#fig.savefig(\"whole_sample_invmass_with_ML.png\")\n",
    "\n",
    "\n",
    "hist1, bin_edges1 = np.histogram(df3['mass'],range=(lowerMassCut, upperMassCut), bins=300)\n",
    "hist2, bin_edges2 = np.histogram(new_check_set['mass'],range=(lowerMassCut, upperMassCut), bins=300)\n",
    "\n",
    "#ratio checked with division by 0\n",
    "ratioBlue = [] #ratio smaller than 1\n",
    "ratioRed = [] # ratio bigger than 1\n",
    "for i in range(len(ns1)):\n",
    "    if (ns1[i] != 0):\n",
    "        ratio = ns[i] / ns1[i]\n",
    "        if (ratio < 1):\n",
    "            ratioBlue.append(ratio)\n",
    "            ratioRed.append(0)\n",
    "        else:\n",
    "            ratioBlue.append(0)\n",
    "            ratioRed.append(ratio)\n",
    "    else:\n",
    "        ratioBlue.append(0)\n",
    "        ratioRed.append(0)\n",
    "#col = []\n",
    "# for val in ratio:\n",
    "#     if val < 1:\n",
    "#         col.append('blue')\n",
    "#     else:\n",
    "#         col.append('red')        \n",
    "axs[1].bar(bins[:-1],     \n",
    "        ratioBlue, \n",
    "        width=0.001,\n",
    "        color = 'blue',\n",
    "        label='ratio < 1')\n",
    "axs[1].bar(bins[:-1],     \n",
    "        ratioRed, \n",
    "        width=0.001,\n",
    "        color = 'red',\n",
    "        label='ratio >= 1')\n",
    "axs[1].legend( loc='upper right')\n",
    "plt.xlabel(\"Mass in $\\dfrac{GeV}{c^2}$\", fontsize = 15)\n",
    "axs[1].set_ylabel(\"XGB / KFPF\", fontsize = 15)\n",
    "axs[1].set_ylim([0.,2.])\n",
    "axs[1].grid()\n",
    "axs[1].tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "plt.show()\n",
    "fig.tight_layout()\n",
    "fig.savefig(directory+'img/xgb_12agev/urqmd/kaon_inv_mass_comparison.png')\n",
    "fig.savefig(directory+'img/xgb_12agev/urqmd/kaon_inv_mass_comparison.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "mask1 = df_clean['xgb_preds']>test_best\n",
    "df3_base=df_clean[mask1]\n",
    "\n",
    "left = .44\n",
    "right = .55\n",
    "range1= (left, right)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 1,figsize=(15,10), sharex=True, gridspec_kw={'width_ratios': [10],\n",
    "                           'height_ratios': [8,4]})\n",
    "\n",
    "ns, bins, patches=axs[0].hist((df3_base['mass']),bins = 50,histtype='step', range=range1,Fill=False, color='red', facecolor='red', linewidth=2)\n",
    "ns1, bins1, patches1=axs[0].hist((new_check_set['mass']),bins = 50,histtype='step', Fill=False, range=range1,facecolor='blue',linewidth=2)\n",
    "#plt.xlabel(\"Mass in GeV\", fontsize = 15)\n",
    "axs[0].set_ylabel(\"log (counts)\", fontsize = 18)\n",
    "#axs[0].grid()\n",
    "axs[0].legend(('XGBoost Selected K-short','KFPF selected K-short'), fontsize = 18, loc='upper right')\n",
    "\n",
    "#plt.rcParams[\"legend.loc\"] = 'upper right'\n",
    "axs[0].set_title(\"The K-short Invariant Mass histogram with KFPF and XGB selection criteria on KFPF variables\", fontsize = 18)\n",
    "#axs[0].grid()\n",
    "axs[0].tick_params(axis='both', which='major', labelsize=18)\n",
    "axs[0].set_yscale('log')\n",
    "#fig.savefig(\"whole_sample_invmass_with_ML.png\")\n",
    "\n",
    "\n",
    "hist1, bin_edges1 = np.histogram(df3_base['mass'],range=(left, right), bins=50)\n",
    "hist2, bin_edges2 = np.histogram(new_check_set['mass'],range=(left, right), bins=50)\n",
    "\n",
    "\n",
    "#makes sense to have only positive values\n",
    "diff = (hist1 - hist2)\n",
    "#axs[1].bar((bins[:-1] + bins[1:]) / 2, # this is what makes it comparable\n",
    "# ns / ns1, # maybe check for div-by-zero!\n",
    "# width=0.001)\n",
    "axs[1].hlines(y=1, xmin=left, xmax=right, colors='r', linestyles='solid', label='')\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "plt.errorbar(center, ns / ns1, fmt='o',\n",
    "                 c='Blue', label='Background in predictions')\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "plt.xlabel(\"Mass in $\\dfrac{GeV}{c^2}$\", fontsize = 18)\n",
    "axs[1].set_ylabel(\"XGB / KFPF\", fontsize = 18)\n",
    "#axs[1].grid()\n",
    "axs[1].tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "plt.show()\n",
    "fig.tight_layout()\n",
    "fig.savefig(directory+'img/xgb_12agev/urqmd/circle_kshort_invmass_with_ML.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del bst_test, df3, df3_base, dtest, dtest1, x_train, y_test, y_train, y_whole\n",
    "gc.collect()\n",
    "backgroundBefore = df_clean[df_clean['issignal']<1]\n",
    "signalBefore = df_clean[df_clean['issignal']==1]\n",
    "backgroundXgb = df_clean[df_clean['xgb_preds1']<1]\n",
    "signalXgb = df_clean[df_clean['xgb_preds1']==1]\n",
    "del df_clean\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signalMan['chi2geo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 654
    },
    "id": "DwAD5im6lIj4",
    "outputId": "3660034f-8f9b-472b-aced-35746f9ad1ee",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_params = PdfPages(directory+'img/xgb_12agev/urqmd/params.pdf') \n",
    "def plotVariables(par, x_label, ranges):\n",
    "\n",
    "    fig, axs = plt.subplots(3, 1,figsize=(15,15))#, sharex=True)\n",
    "    bckgrBefore = backgroundBefore[par]\n",
    "    signBefore = signalBefore[par]\n",
    "    bckgrXgb = backgroundXgb[par]\n",
    "    signXgb = signalXgb[par]\n",
    "\n",
    "    def subAxis(df1, df2, i, title, ranges=ranges, leg1='background',  leg2='signal', bins=100):\n",
    "        axs[i].hist(df1,bins = bins, facecolor='blue',alpha = 0.6, histtype='step', fill=False, linewidth=2, range=ranges) \n",
    "        axs[i].hist(df2,bins = bins, facecolor='red', alpha = 0.7, histtype='step', fill=False, linewidth=2, range=ranges)\n",
    "        axs[i].grid()\n",
    "        axs[i].set_xlabel(x_label, fontsize = 15, loc='right')\n",
    "        axs[i].set_ylim(bottom=1)\n",
    "        axs[i].set_ylabel(\"counts\", fontsize = 15)\n",
    "        axs[i].set_yscale('log')\n",
    "        axs[i].legend((leg1,leg2), fontsize = 15, loc='upper right')\n",
    "        axs[i].set_title(title, fontsize = 15)\n",
    "\n",
    "    subAxis(bckgrBefore,signBefore, 0, 'before selection') \n",
    "    subAxis(bckgrXgb,signXgb, 1, 'XGB selection') \n",
    "    subAxis(signBefore,signXgb, 2, 'signal before and after XGB', leg1='signal before XGB', leg2='signal after XGB', bins=300) \n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(directory+'img/xgb_12agev/urqmd/' + par + '.png')\n",
    "    fig.savefig(pdf_params,format='pdf')\n",
    "    fig.subplots_adjust(top=0.93) #for overall title\n",
    "    fig.suptitle(par, fontsize = 18)\n",
    "    plt.show()\n",
    "#loverdl\n",
    "par = 'loverdl'\n",
    "x_label = r'$\\dfrac{l}{\\Delta l}$'\n",
    "ranges = (0,10000)\n",
    "plotVariables(par, x_label, ranges)\n",
    "#distance\n",
    "par = 'distance'\n",
    "x_label = 'DCA'\n",
    "ranges = (0,5)\n",
    "plotVariables(par, x_label, ranges)\n",
    "#chi2topo\n",
    "par = 'chi2topo'\n",
    "x_label = r'$\\chi^2_{topo}$'\n",
    "ranges = (0,100)\n",
    "plotVariables(par, x_label, ranges)\n",
    "#chi2geo\n",
    "par = 'chi2geo'\n",
    "x_label = r'$\\chi^2_{geo}$'\n",
    "plotVariables(par, x_label, ranges)\n",
    "#chi2primfirst\n",
    "par = 'chi2primfirst'\n",
    "x_label = r'$\\chi^2_{prim} first$'\n",
    "plotVariables(par, x_label, ranges)\n",
    "#chi2primsecond\n",
    "par = 'chi2primsecond'\n",
    "x_label = r'$\\chi^2_{prim} second$'\n",
    "plotVariables(par, x_label, ranges)\n",
    "#mass\n",
    "par = 'mass'\n",
    "ranges = (lowerMassCut,upperMassCut)\n",
    "x_label = r'$mass$'\n",
    "plotVariables(par, x_label, ranges)\n",
    "#pZ\n",
    "par = 'pz'\n",
    "ranges = (pzLowerCut,10)\n",
    "x_label = r'$pZ$'\n",
    "plotVariables(par, x_label, ranges)\n",
    "#pZ\n",
    "par = 'p'\n",
    "ranges = (-1,10)\n",
    "x_label = r'$p$'\n",
    "plotVariables(par, x_label, ranges)\n",
    "#rapidity\n",
    "par = 'rapidity'\n",
    "ranges = (0,4)\n",
    "x_label = r'$rapidity$'\n",
    "plotVariables(par, x_label, ranges)\n",
    "#rapidity\n",
    "par = 'eta'\n",
    "ranges = (lowerEtaCut,upperEtaCut)\n",
    "x_label = r'$\\eta$'\n",
    "plotVariables(par, x_label, ranges)\n",
    "pdf_params.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4qfipIF1f9C"
   },
   "source": [
    "# Importing the final predictor to root\n",
    "We will use the treelite library to transport the final predictor of our model to root"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EclXZNYd11Dc",
    "outputId": "0e3ba6c8-0ba4-4d77-fbd2-31beff07fcda"
   },
   "source": [
    "#install treelite \n",
    "import treelite\n",
    "#create an object out of your model, bst in our case\n",
    "model = treelite.Model.from_xgboost(bst)\n",
    "#use GCC compiler\n",
    "toolchain = 'gcc'\n",
    "#parallel_comp can be changed upto as many processors as one have\n",
    "model.export_lib(toolchain=toolchain, libpath='./mymodel.so',\n",
    "                 params={'parallel_comp': 8}, verbose=True)\n",
    "\n",
    "# Operating system of the target machine\n",
    "platform = 'unix'\n",
    "# C compiler to use to compile prediction code on the target machine\n",
    "toolchain = 'gcc'\n",
    "# Save the source package as a zip archive named mymodel.zip\n",
    "# Later, we'll use this package to produce the library mymodel.so.\n",
    "model.export_srcpkg(platform=platform, toolchain=toolchain,\n",
    "                    pkgpath='./mymodel.zip', libname='mymodel.so',\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ha8m3Kub6UAt",
    "outputId": "8524aa6c-7c50-45ae-a3cf-42d9b8ab1cab"
   },
   "source": [
    "!unzip mymodel.zip"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o4DPuQAQAGVS",
    "outputId": "e25d713f-3048-4a04-b5d1-8352c7394609"
   },
   "source": [
    "#Build the source package (using GNU Make or NMake).\n",
    "%cd mymodel \n",
    "!make\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "cXGt5x8fpZBu"
   },
   "source": [
    "If one wants to transfer this model to a different computer, target machine, then one should follow the following commands in command prompt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "VoJldomSp-bu"
   },
   "source": [
    "sftp john@lxpool.gsi.de\n",
    "sftp> put mymodel.zip\n",
    "sftp> quit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "IjOvqUdRqNvA"
   },
   "source": [
    "ssh john@lxpool.gsi.de\n",
    "unzip mymodel.zip\n",
    "cd mymodel\n",
    "make -j8\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "ZpKXRlds6jpD"
   },
   "source": [
    "The following C++ code can be the applied as a macro on some new data file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4pYC2l_r6iuO",
    "outputId": "025d02e1-2310-4156-ea1e-6a9904498907"
   },
   "source": [
    "%%writefile test.cpp\n",
    "#include \"TROOT.h\"\n",
    "#include \"TFile.h\"\n",
    "#include \"TTree.h\"\n",
    "#include \"TH1F.h\"\n",
    "#include <vector>\n",
    "#include \"TString.h\"\n",
    "#include \"TMath.h\"\n",
    "#include \"TH1.h\"\n",
    "#include \"TF1.h\"\n",
    "#include \"TCanvas.h\"\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include \"main.c\"\n",
    "\n",
    "\n",
    "\n",
    "void python_to_root(){\n",
    "  TCanvas* c1 = new TCanvas();\n",
    "  //c1->Divide(2,1);\n",
    "  \n",
    "  //c1->cd(1);\n",
    "  \n",
    "  /* Open the root file*/\n",
    "  TFile *f = new TFile(\"/gdrive/My Drive/presentations/treelite/10k_events_PFSimplePlainTree.root\",\"UPDATE\");\n",
    "  TTree *t1 = (TTree*)f->Get(\"PlainTree\");\n",
    "  \n",
    "  \n",
    "  Float_t  LambdaCandidates_chi2primneg, LambdaCandidates_chi2primpos, LambdaCandidates_ldl, LambdaCandidates_distance, LambdaCandidates_chi2geo, LambdaCandidates_mass;\n",
    "  \n",
    "  t1->SetBranchAddress(\"LambdaCandidates_chi2primneg\",&LambdaCandidates_chi2primneg);\n",
    "  t1->SetBranchAddress(\"LambdaCandidates_chi2primpos\",&LambdaCandidates_chi2primpos);\n",
    "  t1->SetBranchAddress(\"LambdaCandidates_ldl\",&LambdaCandidates_ldl);\n",
    "  t1->SetBranchAddress(\"LambdaCandidates_distance\",&LambdaCandidates_distance);\n",
    "  t1->SetBranchAddress(\"LambdaCandidates_chi2geo\",&LambdaCandidates_chi2geo);\n",
    "  t1->SetBranchAddress(\"LambdaCandidates_mass\",&LambdaCandidates_mass);\n",
    "  \n",
    "  \n",
    "  std::vector<float> output{};\n",
    "  \n",
    "  const long n_entries = t1->GetEntries();\n",
    "  output.reserve(n_entries);\n",
    "\n",
    "  const size_t n_features = 5;\n",
    "  union Entry input[n_features];\n",
    "  \n",
    "  //let's create a new branch which will store the probability returened for each proton-pion pair by our model \n",
    "  Float_t probab;\n",
    "  TBranch *bpt = t1->Branch(\"probab\",&probab,\"probabilities\");\n",
    "  t1->SetBranchAddress(\"probab\",&probab);\n",
    "  \n",
    "  TH1F *h = new TH1F(\"h_prob\", \"This is the #Lambda invariant mass after the cut >0.918080\", 300, 1.04, 1.4);\n",
    "  h->GetXaxis()->SetTitle(\"[GeV]\");\n",
    "  \n",
    "  for (long i = 0; i < n_entries; i++)\n",
    "   {\n",
    "     t1->GetEntry(i);\n",
    "    \n",
    "    input[0].fvalue = LambdaCandidates_chi2primneg;\n",
    "    input[1].fvalue = LambdaCandidates_chi2primpos;\n",
    "    input[2].fvalue = LambdaCandidates_ldl;\n",
    "    input[3].fvalue = LambdaCandidates_distance;\n",
    "    input[4].fvalue = LambdaCandidates_chi2geo;\n",
    "    \n",
    "    output.push_back(predict(input, 0));\n",
    "    probab= output.at(i);\n",
    "   \n",
    "    if(probab > 0.918080){\n",
    "    h->Fill(LambdaCandidates_mass);}\n",
    "  }\n",
    "  \n",
    "  h->Draw();\n",
    "\n",
    "  \n",
    "  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "yjofU-aMx8O7"
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nkyy2xmSxb2g",
    "outputId": "f6dfac3c-a3b8-47ce-9d0b-d8638f708199"
   },
   "source": [
    "# Installing root\n",
    "!mkdir -p APPS\n",
    "!pwd\n",
    "!cd APPS && wget https://root.cern/download/root_v6.22.00.Linux-ubuntu19-x86_64-gcc9.2.tar.gz \n",
    "!cd APPS && tar -xf root_v6.22.00.Linux-ubuntu19-x86_64-gcc9.2.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of XGB for Collaboration Meeting clean.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

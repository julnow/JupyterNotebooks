{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables of filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLAINTREE FILES NAMES\n",
    "directory='/Users/julnow/gsi/cbm/ML/JupyterNotebooks/'\n",
    "signalFileName = directory + 'PlainTree_100kSign.root'\n",
    "backgroundFileName = directory + 'PlainTree_20kBckgr.root'\n",
    "allFileName = directory + 'PlainTree_20kAll.root'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "wXUjK-nyW4xt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'CBM_ML_Lambda_Library'...\n",
      "remote: Enumerating objects: 88, done.\u001b[K\n",
      "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
      "remote: Compressing objects: 100% (81/81), done.\u001b[K\n",
      "remote: Total 88 (delta 37), reused 0 (delta 0), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (88/88), 28.22 KiB | 3.13 MiB/s, done.\n",
      "Resolving deltas: 100% (37/37), done.\n",
      "/Users/julnow/gsi/cbm/ML/JupyterNotebooks/CBM_ML_Lambda_Library/mymodel/CBM_ML_Lambda_Library\n",
      "\u001b[33mhint: Pulling without specifying how to reconcile divergent branches is\u001b[m\n",
      "\u001b[33mhint: discouraged. You can squelch this message by running one of the following\u001b[m\n",
      "\u001b[33mhint: commands sometime before your next pull:\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint:   git config pull.rebase false  # merge (the default strategy)\u001b[m\n",
      "\u001b[33mhint:   git config pull.rebase true   # rebase\u001b[m\n",
      "\u001b[33mhint:   git config pull.ff only       # fast-forward only\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint: You can replace \"git config\" with \"git config --global\" to set a default\u001b[m\n",
      "\u001b[33mhint: preference for all repositories. You can also pass --rebase, --no-rebase,\u001b[m\n",
      "\u001b[33mhint: or --ff-only on the command line to override the configured default per\u001b[m\n",
      "\u001b[33mhint: invocation.\u001b[m\n",
      "From https://github.com/shahidzk1/CBM_ML_Lambda_Library\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n",
      "Requirement already satisfied: uproot in /opt/homebrew/Caskroom/miniforge/base/envs/cbm21/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (4.0.11)\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/Caskroom/miniforge/base/envs/cbm21/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (3.4.2)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniforge/base/envs/cbm21/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniforge/base/envs/cbm21/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.21.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniforge/base/envs/cbm21/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (0.24.2)\n",
      "Requirement already satisfied: xgboost in /opt/homebrew/Caskroom/miniforge/base/envs/cbm21/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (0.80)\n",
      "Requirement already satisfied: bayesian-optimization in /opt/homebrew/Caskroom/miniforge/base/envs/cbm21/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (1.2.0)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /opt/homebrew/Caskroom/miniforge/base/envs/cbm21/lib/python3.8/site-packages (from bayesian-optimization->-r requirements.txt (line 7)) (1.7.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/homebrew/Caskroom/miniforge/base/envs/cbm21/lib/python3.8/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/cbm21/lib/python3.8/site-packages (from scikit-learn->-r requirements.txt (line 5)) (2.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/homebrew/Caskroom/miniforge/base/envs/cbm21/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 2)) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/cbm21/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 2)) (8.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/Caskroom/miniforge/base/envs/cbm21/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/Caskroom/miniforge/base/envs/cbm21/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/Caskroom/miniforge/base/envs/cbm21/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 2)) (0.10.0)\n",
      "Requirement already satisfied: six in /opt/homebrew/Caskroom/miniforge/base/envs/cbm21/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->-r requirements.txt (line 2)) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/homebrew/Caskroom/miniforge/base/envs/cbm21/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 3)) (2021.1)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Caskroom/miniforge/base/envs/cbm21/lib/python3.8/site-packages (from uproot->-r requirements.txt (line 1)) (57.4.0)\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "creating CBM_ML.egg-info\n",
      "writing CBM_ML.egg-info/PKG-INFO\n",
      "writing dependency_links to CBM_ML.egg-info/dependency_links.txt\n",
      "writing top-level names to CBM_ML.egg-info/top_level.txt\n",
      "writing manifest file 'CBM_ML.egg-info/SOURCES.txt'\n",
      "reading manifest file 'CBM_ML.egg-info/SOURCES.txt'\n",
      "writing manifest file 'CBM_ML.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.macosx-11.1-arm64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "creating build/lib/CBM_ML\n",
      "copying CBM_ML/KFPF_lambda_cuts.py -> build/lib/CBM_ML\n",
      "copying CBM_ML/__init__.py -> build/lib/CBM_ML\n",
      "copying CBM_ML/plot_tools.py -> build/lib/CBM_ML\n",
      "copying CBM_ML/root_functions.py -> build/lib/CBM_ML\n",
      "copying CBM_ML/tree_importer.py -> build/lib/CBM_ML\n",
      "creating build/bdist.macosx-11.1-arm64\n",
      "creating build/bdist.macosx-11.1-arm64/egg\n",
      "creating build/bdist.macosx-11.1-arm64/egg/CBM_ML\n",
      "copying build/lib/CBM_ML/KFPF_lambda_cuts.py -> build/bdist.macosx-11.1-arm64/egg/CBM_ML\n",
      "copying build/lib/CBM_ML/__init__.py -> build/bdist.macosx-11.1-arm64/egg/CBM_ML\n",
      "copying build/lib/CBM_ML/plot_tools.py -> build/bdist.macosx-11.1-arm64/egg/CBM_ML\n",
      "copying build/lib/CBM_ML/root_functions.py -> build/bdist.macosx-11.1-arm64/egg/CBM_ML\n",
      "copying build/lib/CBM_ML/tree_importer.py -> build/bdist.macosx-11.1-arm64/egg/CBM_ML\n",
      "byte-compiling build/bdist.macosx-11.1-arm64/egg/CBM_ML/KFPF_lambda_cuts.py to KFPF_lambda_cuts.cpython-38.pyc\n",
      "byte-compiling build/bdist.macosx-11.1-arm64/egg/CBM_ML/__init__.py to __init__.cpython-38.pyc\n",
      "byte-compiling build/bdist.macosx-11.1-arm64/egg/CBM_ML/plot_tools.py to plot_tools.cpython-38.pyc\n",
      "byte-compiling build/bdist.macosx-11.1-arm64/egg/CBM_ML/root_functions.py to root_functions.cpython-38.pyc\n",
      "byte-compiling build/bdist.macosx-11.1-arm64/egg/CBM_ML/tree_importer.py to tree_importer.cpython-38.pyc\n",
      "creating build/bdist.macosx-11.1-arm64/egg/EGG-INFO\n",
      "copying CBM_ML.egg-info/PKG-INFO -> build/bdist.macosx-11.1-arm64/egg/EGG-INFO\n",
      "copying CBM_ML.egg-info/SOURCES.txt -> build/bdist.macosx-11.1-arm64/egg/EGG-INFO\n",
      "copying CBM_ML.egg-info/dependency_links.txt -> build/bdist.macosx-11.1-arm64/egg/EGG-INFO\n",
      "copying CBM_ML.egg-info/top_level.txt -> build/bdist.macosx-11.1-arm64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "creating dist\n",
      "creating 'dist/CBM_ML-0.1.0-py3.8.egg' and adding 'build/bdist.macosx-11.1-arm64/egg' to it\n",
      "removing 'build/bdist.macosx-11.1-arm64/egg' (and everything under it)\n",
      "Processing CBM_ML-0.1.0-py3.8.egg\n",
      "Removing /opt/homebrew/Caskroom/miniforge/base/envs/cbm21/lib/python3.8/site-packages/CBM_ML-0.1.0-py3.8.egg\n",
      "Copying CBM_ML-0.1.0-py3.8.egg to /opt/homebrew/Caskroom/miniforge/base/envs/cbm21/lib/python3.8/site-packages\n",
      "CBM-ML 0.1.0 is already the active version in easy-install.pth\n",
      "\n",
      "Installed /opt/homebrew/Caskroom/miniforge/base/envs/cbm21/lib/python3.8/site-packages/CBM_ML-0.1.0-py3.8.egg\n",
      "Processing dependencies for CBM-ML==0.1.0\n",
      "Finished processing dependencies for CBM-ML==0.1.0\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/shahidzk1/CBM_ML_Lambda_Library.git\n",
    "%cd CBM_ML_Lambda_Library\n",
    "!git pull origin main\n",
    "!pip install -r requirements.txt\n",
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "UXNBDcG472oy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n",
    "#from hipe4ml.model_handler import ModelHandler\n",
    "#from hipe4ml.tree_handler import TreeHandler\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "#from hipe4ml import plot_utils\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.stats import uniform\n",
    "\n",
    "from numpy import sqrt, log, argmax\n",
    "\n",
    "import weakref \n",
    "import itertools\n",
    "\n",
    "from CBM_ML import tree_importer,  KFPF_lambda_cuts, plot_tools\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import gc, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4Ie_PTM72qH"
   },
   "source": [
    "# Selecting Background and Signal\n",
    "We generate PlainTrees from: 100k events (signal) and 20k evenents (background) with Au-Au @12A GeV/c, DCMQGSM-SMM generator passed through CBM setup in GEANT4, without any cuts\n",
    "\n",
    "To omit imbalance classification problem (event though there are 5 times more events for background, the number of entries is much smaller for signal) we resample the data. We're deleting instances from the over-represented class (in our case the background) - it's called under-sampling, one of the resampling methods. \n",
    "\n",
    "So for training and testing we will get signal candidates from the 5 sigma region and  background from outside this region (3 times signal size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUTS FOR MASS SELECTION\n",
    "#5 sigma region for signal\n",
    "lower5SigmaCutSign = 0.43485\n",
    "upper5SigmaCutSign = 0.56135\n",
    "#mean invariant mass\n",
    "invMass = 0.4981\n",
    "# \"5sigma\" (not acutal 5 sigma) region for background\n",
    "lower5SigmaCutBckgr = 0.1\n",
    "upper5SigmaCutBckgr = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKnmnX_TwAig"
   },
   "outputs": [],
   "source": [
    "# We import three root files into our jupyter notebook\n",
    "#sign - before cleaning but after 5sigma seletion, signal - cleaned\n",
    "#so that we dont have to reimport everything with each change\n",
    "sign = tree_importer.tree_importer(signalFileName,'PlainTree',7)\n",
    "\n",
    "# We only select lambda candidates in the 5 sigma region around the kaon mass peak\n",
    "#we preserve the cleaned dataframe with a changed name\n",
    "sign = sign[(sign['Candidates_generation']==1) & (sign['Candidates_mass']>lower5SigmaCutSign) & (sign['Candidates_mass']<upper5SigmaCutSign)]\n",
    "\n",
    "# Similarly for the background, we select background candidates which are not in the 5 sigma region of the kaon peak\n",
    "bckgr = tree_importer.tree_importer(backgroundFileName,'PlainTree',7)\n",
    "#we preserve the cleaned dataframe with a changed name\n",
    "bckgr = bckgr[(bckgr['Candidates_generation'] < 1)\n",
    "                 & ((bckgr['Candidates_mass'] > lower5SigmaCutBckgr)\n",
    "                 & (bckgr['Candidates_mass'] < lower5SigmaCutSign) | (bckgr['Candidates_mass']>upper5SigmaCutSign) \n",
    "                    & (bckgr['Candidates_mass'] < upper5SigmaCutBckgr))\n",
    "             ].sample(n=5*(sign.shape[0])) #we select bckgr so that we have 5*more entries that for signal\n",
    "\n",
    "#Also call the garbage collector of python to collect unused items to free memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#we remove name prefixes 'Candidates' and do some renaming\n",
    "bckgr.columns = bckgr.columns.str.replace('Candidates_', '')\n",
    "bckgr.columns = bckgr.columns.str.replace('_', '')\n",
    "sign.columns = sign.columns.str.replace('Candidates_', '')\n",
    "sign.columns = sign.columns.str.replace('_', '')\n",
    "#we also get rid of coordinates errors\n",
    "sign = sign.drop(columns=['xerror', 'yerror', 'zerror', 'daughter1id', 'daughter2id', 'pid', 'pTerr', 'etaerr', 'masserr', 'phierr']).rename(columns={'generation' : 'issignal'})\n",
    "bckgr = bckgr.drop(columns=['xerror', 'yerror', 'zerror', 'daughter1id', 'daughter2id', 'pid', 'pTerr', 'etaerr', 'masserr', 'phierr']).rename(columns={'generation' : 'issignal'})\n",
    "#let's check the name prefixes \n",
    "sign.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSmy1dEU72pz"
   },
   "source": [
    "The label 'issignal' tells us if an entry comes from signal (1) or background (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WHuzi6GxB1L"
   },
   "source": [
    "# Data Cleaning\n",
    "Sometimes a data set contains entries which are outliers or does not make sense. For example, infinite values or NaN entries. We clean the data by removing these entries. \n",
    "\n",
    "Similarly, CBM is a fixed target experiment so there are certain conditions which the data has to satisfy before it is considered as reliable data.So we apply certain limits on the data sets.\n",
    "\n",
    "The values of these cuts are described: https://drive.google.com/file/d/1tb0FBRq4KgVu-VQZgpjA8ONbIGVCNOnE/view?usp=sharing https://github.com/julnow/JupyterNotebooks/blob/kaon/CBM%20K-short%20data%20cleaning.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUTS USED FOR DATA CLEANING\n",
    "#mass cuts for both bckgr and sign\n",
    "lowerMassCut = 0.279\n",
    "upperMassCut = 1.5\n",
    "#distance cuts\n",
    "#DCA\n",
    "lowerDcaCut = 0\n",
    "upperDcaCut = 100\n",
    "#l distance\n",
    "lowerLCut = 0\n",
    "upperLCut = 80\n",
    "#loverdl\n",
    "lowerLdlCut = 0\n",
    "upperLdlCut = 15000\n",
    "#coordinate cuts\n",
    "absXCut = 50\n",
    "absYCut = 50\n",
    "lowerZCut = -1\n",
    "upperZCut = 80\n",
    "#momentums cuts\n",
    "pzLowerCut = 0\n",
    "pUpperCut = 20\n",
    "ptUpperCut = 3\n",
    "#chi2\n",
    "#geo\n",
    "lowerChi2GeoCut = 0\n",
    "upperChi2GeoCut = 10000\n",
    "#topo\n",
    "lowerChi2TopoCut = 0\n",
    "upperChi2TopoCut = 100000\n",
    "#prim first\n",
    "lowerChi2PrimFirstCut = 0\n",
    "upperChi2PrimFirstCut = 3e7\n",
    "#prim second\n",
    "lowerChi2PrimSecondCut = 0\n",
    "upperChi2PrimSecondCut = 3e7\n",
    "#pseudorapidity cuts\n",
    "lowerEtaCut = 1.\n",
    "upperEtaCut = 6.5#CUTS USED FOR DATA CLEANING\n",
    "#mass cuts for both bckgr and sign\n",
    "lowerMassCut = 0.279\n",
    "upperMassCut = 1.5\n",
    "#distance cuts\n",
    "#DCA\n",
    "lowerDcaCut = 0\n",
    "upperDcaCut = 100\n",
    "#l distance\n",
    "lowerLCut = 0\n",
    "upperLCut = 80\n",
    "#loverdl\n",
    "lowerLdlCut = 0\n",
    "upperLdlCut = 15000\n",
    "#coordinate cuts\n",
    "absXCut = 50\n",
    "absYCut = 50\n",
    "lowerZCut = -1\n",
    "upperZCut = 80\n",
    "#momentums cuts\n",
    "pzLowerCut = 0\n",
    "pUpperCut = 20\n",
    "ptUpperCut = 3\n",
    "#chi2\n",
    "#geo\n",
    "lowerChi2GeoCut = 0\n",
    "upperChi2GeoCut = 10000\n",
    "#topo\n",
    "lowerChi2TopoCut = 0\n",
    "upperChi2TopoCut = 100000\n",
    "#prim first\n",
    "lowerChi2PrimFirstCut = 0\n",
    "upperChi2PrimFirstCut = 3e7\n",
    "#prim second\n",
    "lowerChi2PrimSecondCut = 0\n",
    "upperChi2PrimSecondCut = 3e7\n",
    "#pseudorapidity cuts\n",
    "lowerEtaCut = 1.\n",
    "upperEtaCut = 6.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEmS1939xEDS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    # let's treat all the infinite, inf, values by nan and then we drop all the null entries\n",
    "    with pd.option_context('mode.use_inf_as_na', True):\n",
    "        df = df.dropna()\n",
    "    #Experimental constraints\n",
    "    is_good_mom = (df['pz'] > pzLowerCut) & (df['p']<pUpperCut) & (df['pT']<ptUpperCut)\n",
    "    is_good_coord = (abs(df['x']) < absXCut) & (abs(df['y']) < absYCut) & (df['z']>lowerZCut) & (df['z']<upperZCut)\n",
    "    is_good_params = (df['distance'] > lowerDcaCut) & (df['distance'] < upperDcaCut) & (df['chi2geo']>lowerChi2GeoCut) & (df['chi2geo'] < upperChi2GeoCut) & (df['chi2topo'] > lowerChi2TopoCut) & (df['chi2topo'] < upperChi2TopoCut) & (df['eta']>lowerEtaCut) & (df['eta']<upperEtaCut)& (df['l']>lowerLCut) & (df['l']<upperLCut) & (df['loverdl']>lowerLdlCut) & (df['loverdl']<upperLdlCut)\n",
    "    is_good_daughters = (df['chi2primfirst']>lowerChi2PrimFirstCut) & (df['chi2primfirst'] < upperChi2PrimSecondCut) & (df['chi2primsecond']>lowerChi2PrimSecondCut) & (df['chi2primsecond']<upperChi2PrimFirstCut)\n",
    "    is_good_mass = (df['mass']>lowerMassCut) & (df['mass']<upperMassCut)\n",
    "\n",
    "    is_good_df = (is_good_mom) & (is_good_coord) & (is_good_params) & (is_good_daughters) & (is_good_mass)\n",
    "\n",
    "    return df[is_good_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpWTegAtxF9F"
   },
   "outputs": [],
   "source": [
    "background = clean_df(bckgr)\n",
    "signal = clean_df(sign)\n",
    "del sign, bckgr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also import and clean dataset of 10k events for both background and signal (for testing our algorhitm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_origin = tree_importer.tree_importer(allFileName,'PlainTree',7)\n",
    "df_origin.columns = df_origin.columns.str.replace('Candidates_', '')\n",
    "df_origin.columns = df_origin.columns.str.replace('_', '')\n",
    "df_origin = df_origin.drop(columns=['xerror', 'yerror', 'zerror', 'daughter1id', 'daughter2id', 'pid', 'pTerr', 'etaerr', 'masserr', 'phierr']).rename(columns={'generation' : 'issignal'})\n",
    "df_clean = clean_df(df_origin)\n",
    "del df_origin\n",
    "gc.collect()\n",
    "#lets look at impoted tree\n",
    "df_clean.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUTTkp2072qI"
   },
   "outputs": [],
   "source": [
    "#Let's combine signal and background\n",
    "df_scaled = pd.concat([signal, background])\n",
    "# Let's shuffle the rows randomly\n",
    "df_scaled = df_scaled.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('size of:\\nsignal: ' + str(len(signal)) + '\\nbackground: ' + str(len(background)) \n",
    "      + '\\nbackground to signal ratio: ' + str(round(len(background)/len(signal), 1))  \n",
    "      + '\\ntest set (both signal and background): ' + str(len(df_clean))\n",
    "      + '\\ntest set (background to signal ratio): ' \n",
    "      + str(round(len(df_clean.loc[df_clean['issignal'] == 0])/len(df_clean.loc[df_clean['issignal'] == 1]), 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4Ie_PTM72qH"
   },
   "source": [
    "# Selecting Background and Signal\n",
    "After cleaning, our training data set contains 238477 background candidates and 89987 signal candidates (background to signal ratio $\\approx 2.7$). For testing, we'll use cleaned dataset containing 34200168 entries of signal and background (background to signal ratio $\\approx 1871.3$)\n",
    "\n",
    "Here, we use under-sampling to tackle the problem with the classification of underrepresented class (normally, the signal is only approx. 0.05% of real data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(directory+'img/xgb'):\n",
    "    os.makedirs(directory+'img/xgb')\n",
    "#lets draw sign to background ratio\n",
    "def plt_sig_back(df):\n",
    "    range1 = (lowerMassCut, upperMassCut)\n",
    "    fig, axs = plt.subplots(figsize=(20, 10))\n",
    "    #df_scaled['mass'].plot.hist(bins = 300, range=range1,grid=True,sharey=True)\n",
    "    (df[df['issignal']==0])['mass'].plot.hist(bins = 300, facecolor='yellow',grid=True,range=range1, label='Background')\n",
    "    (df[df['issignal']==1])['mass'].plot.hist(bins = 300, facecolor='magenta',grid=True, range=range1, label ='Signal')\n",
    "    #plt.vlines(x=1.108,ymin=-1,ymax=48000, color='black', linestyle='-')\n",
    "    #plt.vlines(x=1.1227,ymin=-1,ymax=48000, color='black', linestyle='-')\n",
    "    plt.ylabel(\"Counts (log scale)\", fontsize=15)\n",
    "    plt.xlabel(\"Mass in GeV/$c^2$\", fontsize= 15)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.title('Test and Train K-short Invariant Mass', fontsize = 15)\n",
    "    plt.legend( fontsize = 15)\n",
    "    axs.tick_params(axis='both', which='major', labelsize=18)\n",
    "    plt.yscale(\"log\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(directory+'img/xgb/inv_mass_trainset.pdf')\n",
    "    fig.savefig(directory+'img/xgb/inv_mass_trainset.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVQErryc72qc"
   },
   "outputs": [],
   "source": [
    "plt_sig_back(df_scaled)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyveX9Lh72qk"
   },
   "source": [
    "# Creating Train and Test sets\n",
    "To make machine learning algorithms more efficient on unseen data we divide our data into two sets. One set is for training the algorithm and the other is for testing the algorithm. If we don't do this then the algorithm can overfit and we will not capture the general trends in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOEQTRhb72ql"
   },
   "outputs": [],
   "source": [
    "# The following columns will be used to predict whether a reconstructed candidate is a lambda particle or not\n",
    "cuts = [ 'loverdl', 'distance', 'chi2geo', 'chi2topo', 'chi2primfirst', 'chi2primsecond']\n",
    "\n",
    "\n",
    "x = df_scaled[cuts].copy()\n",
    "\n",
    "# The MC information is saved in this y variable\n",
    "y =pd.DataFrame(df_scaled['issignal'], dtype='int')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=324)\n",
    "\n",
    "#DMatrix is a internal data structure that used by XGBoost which is optimized for both memory efficiency and training speed. \n",
    "dtrain = xgb.DMatrix(x_train, label = y_train)\n",
    "dtest=xgb.DMatrix(x_test, label = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole set\n",
    "We also select the selected variables from the 10k events data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole = df_clean[cuts].copy()\n",
    "y_whole = pd.DataFrame(df_clean['issignal'], dtype='int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOO7bZaY72q8"
   },
   "source": [
    "<p style=\"font-family: Arial; font-size:3em;color:purple; font-style:bold\"><br>XGB Boost \n",
    "<br></p><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27CMYzmu72q-"
   },
   "outputs": [],
   "source": [
    "#DMatrix is a internal data structure that used by XGBoost which is optimized for both memory efficiency and training speed. \n",
    "dtrain = xgb.DMatrix(x_train, label = y_train)\n",
    "dtest = xgb.DMatrix(x_whole, label = y_whole)\n",
    "dtest1=xgb.DMatrix(x_test, label = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FihmBwJ72rE"
   },
   "source": [
    "## Bayesian optimization\n",
    "In order to find the best parameters of XGB for our data we use Bayesian optimization. Grid search and and random search could also do the same job but bayesian is more time efficient. For further reading visit [the git page](https://github.com/fmfn/BayesianOptimization) of the bayesian optimization used here.\n",
    "\n",
    "### Hyper parameters\n",
    "Some of the following hyper parameters will be tuned for our algorithm:\n",
    "\n",
    "\n",
    "*subsample* [default=1]\n",
    "Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.\n",
    "range: (0,1]\n",
    "\n",
    "*eta* [default=0.3, alias: learning_rate]\n",
    "Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n",
    "range: [0,1]\n",
    "\n",
    "\n",
    "*gamma* [default=0, alias: min_split_loss]\n",
    "Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n",
    "range: [0,∞]\n",
    "\n",
    "\n",
    "*alpha* [default=0, alias: reg_alpha]\n",
    "L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "\n",
    "*Lasso Regression* (Least Absolute Shrinkage and Selection Operator) adds “absolute value of magnitude” of coefficient as penalty term to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wL9CUKGT72rc"
   },
   "outputs": [],
   "source": [
    "#Bayesian Optimization function for xgboost\n",
    "#specify the parameters you want to tune as keyword arguments\n",
    "def bo_tune_xgb(max_depth, gamma, alpha, n_estimators ,learning_rate):\n",
    "    params = {'max_depth': int(max_depth),\n",
    "              'gamma': gamma,\n",
    "              'alpha':alpha,\n",
    "              'n_estimators': n_estimators,\n",
    "              'learning_rate':learning_rate,\n",
    "              'subsample': 0.8,\n",
    "              'eta': 0.1,\n",
    "              'eval_metric': 'auc', 'nthread' : 7}\n",
    "    cv_result = xgb.cv(params, dtrain, num_boost_round=70, nfold=5)\n",
    "    return  cv_result['test-auc-mean'].iloc[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eN20_fJV72rg"
   },
   "outputs": [],
   "source": [
    "#Invoking the Bayesian Optimizer with the specified parameters to tune\n",
    "xgb_bo = BayesianOptimization(bo_tune_xgb, {'max_depth': (4, 10),\n",
    "                                             'gamma': (0, 1),\n",
    "                                            'alpha': (2,20),\n",
    "                                             'learning_rate':(0,1),\n",
    "                                             'n_estimators':(100,500)\n",
    "                                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OW00bW2i72rn",
    "outputId": "7bc098cd-98e6-4e2a-a689-75b55a4dc004"
   },
   "outputs": [],
   "source": [
    "#performing Bayesian optimization for 5 iterations with 8 steps of random exploration with an #acquisition function of expected improvement\n",
    "xgb_bo.maximize(n_iter=5, init_points=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebJXjFdP72rv"
   },
   "outputs": [],
   "source": [
    "# best target so far 0.997457\n",
    "print(xgb_bo.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOauOey772r3"
   },
   "source": [
    "# XGB models\n",
    "Now let's take the parameters selected by the bayesian optimization and apply them in our training and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8eiJzKwpwXJ"
   },
   "outputs": [],
   "source": [
    "max_param = xgb_bo.max['params']\n",
    "param= {'alpha': max_param['alpha'], 'gamma': max_param['gamma'], 'learning_rate': max_param['learning_rate'], 'max_depth': int(round(max_param['max_depth'],0)), 'n_estimators': int(round(max_param['n_estimators'],0)), 'objective': 'reg:logistic'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OiTX1iSx72r9",
    "outputId": "01ab4a24-aa9e-4f00-8322-31f0f63ac224"
   },
   "outputs": [],
   "source": [
    "#To train the algorithm using the parameters selected by bayesian optimization\n",
    "bst = xgb.train(param, dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6U8lnP1jJzI"
   },
   "outputs": [],
   "source": [
    "#We apply our model on the training data that was trained on the training data, this helps us to control overfitting\n",
    "bst1= bst.predict(dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2p8YPY1jRQ_"
   },
   "outputs": [],
   "source": [
    "# We apply our trained model on test data and store the predictions in a bst_test dataframe\n",
    "bst_test = pd.DataFrame(data=bst.predict(dtest1),  columns=[\"xgb_preds\"])\n",
    "y_test=y_test.set_index(np.arange(0,bst_test.shape[0]))\n",
    "# We also store the MC information in this dataFrame\n",
    "bst_test['issignal']=y_test['issignal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "5Xm-sY3W72s9",
    "outputId": "878e260c-66bc-4571-f7ef-33a5a7084e09"
   },
   "outputs": [],
   "source": [
    "#The following graph will show us that which features are important for the model\n",
    "ax = xgb.plot_importance(bst)\n",
    "plt.rcParams['figure.figsize'] = [5, 3]\n",
    "plt.show()\n",
    "ax.figure.tight_layout() \n",
    "ax.figure.savefig(directory+'img/xgb/feature_importance.pdf')\n",
    "ax.figure.savefig(directory+'img/xgb/feature_importance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "To2uJSUWAESb"
   },
   "source": [
    "## AUC and ROC\n",
    "\n",
    "The function roc_curve computes the receiver operating characteristic curve, or ROC curve. Quoting Wikipedia :\n",
    "\n",
    "“A receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate.”\n",
    "\n",
    "This function requires the true binary value and the target scores, which can either be probability estimates of the positive class, confidence values, or binary decisions.\n",
    "\n",
    "Similarly, the function roc_auc_score computes Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YV_FtX0MUVj-"
   },
   "source": [
    "To find the best threshold which results more signal to background ratio for lambda candidates we use the parameter S0 called the approximate median significance by the higgs boson  ML challenge (http://higgsml.lal.in2p3.fr/documentation,9.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-jWtHorkGQ9"
   },
   "outputs": [],
   "source": [
    "def AMS(y_true, y_predict, y_true1, y_predict1):\n",
    "    roc_auc=roc_auc_score(y_true, y_predict)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_predict,drop_intermediate=False ,pos_label=1)\n",
    "    S0 = sqrt(2 * ((tpr + fpr) * log((1 + tpr/fpr)) - tpr))\n",
    "    S0 = S0[~np.isnan(S0)]\n",
    "    xi = argmax(S0)\n",
    "    S0_best_threshold = (thresholds[xi])\n",
    "\n",
    "    roc_auc1=roc_auc_score(y_true1, y_predict1)\n",
    "    fpr1, tpr1, thresholds1 = roc_curve(y_true1, y_predict1,drop_intermediate=False ,pos_label=1)\n",
    "    S01 = sqrt(2 * ((tpr1 + fpr1) * log((1 + tpr1/fpr1)) - tpr1))\n",
    "    S01 = S01[~np.isnan(S01)]\n",
    "    xi1 = argmax(S01)\n",
    "    S0_best_threshold1 = (thresholds[xi1])\n",
    "\n",
    "    #plotting\n",
    "    fig, axs = plt.subplots(figsize=(15, 10), dpi = 100)\n",
    "    plt.plot(fpr, tpr, linestyle=':',color='darkorange',label='ROC curve train (area = %0.4f)' % roc_auc)\n",
    "    plt.plot(fpr1, tpr1, color='green',label='ROC curve test (area = %0.4f)' % roc_auc1)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.scatter(fpr[xi], tpr[xi], marker='o', color='black', label= 'Best Threshold train set = '+\"%.4f\" % S0_best_threshold +'\\n S0 = '+ \"%.2f\" % S0[xi])\n",
    "    plt.scatter(fpr1[xi1], tpr1[xi1], marker='o', color='blue', label= 'Best Threshold test set = '+\"%.4f\" % S0_best_threshold1 +'\\n S0 = '+ \"%.2f\" % S01[xi1])\n",
    "    plt.xlabel('False Positive Rate', fontsize = 15)\n",
    "    plt.ylabel('True Positive Rate', fontsize = 15)\n",
    "    plt.legend(loc=\"lower right\", fontsize = 15)\n",
    "    plt.title('Receiver operating characteristic', fontsize = 15)\n",
    "    plt.xlim([-0.01, 1.0])\n",
    "    plt.ylim([0, 1.02])\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig(directory+'img/xgb/ams.pdf')\n",
    "    fig.savefig(directory+'img/xgb/ams.png')\n",
    "\n",
    "    return S0_best_threshold, S0_best_threshold1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 933
    },
    "id": "JvOr9TwvkIZe",
    "outputId": "e70284c8-182b-4a9f-bc40-660372557382"
   },
   "outputs": [],
   "source": [
    "train_best, test_best = AMS(y_train, bst1, y_test, bst_test['xgb_preds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOjnbDBbpB8r"
   },
   "source": [
    "When the AUC, best threshold and approximate median significance for train and test are nearly the same, we save that model and use it. This means that our model is general enough. In my opinion, if the test S0 is above 3.0 then it is a good enough model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the model on the 10k events data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['xgb_preds'] = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['xgb_preds'].hist(bins=300)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loqnFETE_1W1"
   },
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A **Confusion Matrix** $C$ is such that $C_{ij}$ is equal to the number of observations known to be in group $i$ and predicted to be in group $j$. Thus in binary classification, the count of true positives is $C_{00}$, false negatives $C_{01}$,false positives is $C_{10}$, and true neagtives is $C_{11}$.\n",
    "\n",
    "If $ y^{'}_{i} $ is the predicted value of the $ i$-th sample and $y_{i}$ is the corresponding true value, then the fraction of correct predictions over $ n_{samples}$ is defined as \n",
    "$$\n",
    "True \\: positives (y,y^{'}) =  \\sum_{i=1}^{n_{samples} } 1 (y^{'}_{i} = y_{i}=1)\n",
    "$$ \n",
    "\n",
    "The following function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQrDUlyr_07i"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',fontsize = 15)\n",
    "    plt.xlabel('Predicted label',fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQrDUlyr_07i"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',fontsize = 15)\n",
    "    plt.xlabel('Predicted label',fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate efficency and false tu tru signal ratio\n",
    "def confustion_stats(df, cm):\n",
    "    all_signals = len(df.loc[df['issignal'] == 1])\n",
    "    true_signal = cm[0][0]\n",
    "    false_signal = cm[1][0]        \n",
    "    reconstructed_signals = true_signal + false_signal\n",
    "    false_to_true_signals = false_signal / true_signal\n",
    "    efficiency = reconstructed_signals / all_signals * 100 #efficency in % for all\n",
    "    efficiency_true = true_signal / all_signals * 100 #efficency in % for all\n",
    "    print(\"Efficiency: \" + str(round(efficiency, 2)) + \"%\")\n",
    "    print(\"Efficiency of true signal candidates reconstruction: \" + str(round(efficiency_true, 2)) + \"%\")\n",
    "    print(\"False tu true reconstructed signal ratio: \" + str(round(false_to_true_signals, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 636
    },
    "id": "QP6WiKgQ_8yG",
    "outputId": "83f8a707-7b05-4ab5-e50f-e9ef69c2cfcd"
   },
   "outputs": [],
   "source": [
    "#lets take the best threshold and look at the confusion matrix\n",
    "cut1 = test_best\n",
    "df_clean['xgb_preds1'] = ((df_clean['xgb_preds']>cut1)*1)\n",
    "cnf_matrix = confusion_matrix(y_whole, df_clean['xgb_preds1'], labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "fig, axs = plt.subplots(figsize=(10, 8))\n",
    "axs.yaxis.set_label_coords(-0.04,.5)\n",
    "axs.xaxis.set_label_coords(0.5,-.005)\n",
    "plot_confusion_matrix(cnf_matrix, classes=['signal','background'], title='Confusion Matrix for XGB for cut > '+str(cut1))\n",
    "fig.savefig(directory+'img/xgb/confusion_matrix_extreme_gradient_boosting_whole_data.pdf')\n",
    "fig.savefig(directory+'img/xgb/confusion_matrix_extreme_gradient_boosting_whole_data.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confustion_stats(df_clean, cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtZSmZ8kAZg5"
   },
   "outputs": [],
   "source": [
    "# The following function will display the inavriant mass histogram of the original 10k event set along with the mass histoigram after we apply a cut\n",
    "# on the probability prediction of xgb\n",
    "def cut_visualization(cut, range1=(lowerMassCut, upperMassCut), bins1= 300 ):\n",
    "    mask1 = df_clean['xgb_preds']>cut\n",
    "    df3=df_clean[mask1]\n",
    "    \n",
    "    fig, ax2 = plt.subplots(figsize=(15, 10), dpi = 200)\n",
    "    color = 'tab:blue'\n",
    "    ax2.hist(df_clean['mass'],bins = bins1, range=range1, facecolor='blue',alpha = 0.35, label='before selection')\n",
    "    ax2.set_ylabel('Counts', fontsize = 15, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.legend( fontsize = 15, loc='upper left')\n",
    "    \n",
    "    color = 'tab:red'\n",
    "    ax1 = ax2.twinx()\n",
    "    ax1.hist(df3['mass'], bins = bins1, range=range1, facecolor='red',alpha = 0.35, label='XGB')\n",
    "    ax1.set_xlabel('Mass in GeV', fontsize = 15)\n",
    "    ax1.set_ylabel('Counts ', fontsize = 15, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.legend( fontsize = 15,loc='upper right' )\n",
    "\n",
    "    plt.title(\"The original sample's Invariant Mass along with mass after selection of XGB (with a cut > \"+str(cut)+')', fontsize = 15)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(directory+'img/xgb/whole_sample_invmass_with_ML.pdf')\n",
    "    fig.savefig(directory+'img/xgb/whole_sample_invmass_with_ML.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JOnsE6BTAfdx",
    "outputId": "f480f13b-c2ce-44c8-a639-cd849766ce80"
   },
   "outputs": [],
   "source": [
    "cut_visualization(test_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrItzy4C72uS"
   },
   "source": [
    "# Comparison with the manually optimized cuts of KFPF\n",
    "In the already existing Kalman Filter Particle Finder (KFPF) package for online reconstruction and selection of short-lived particles in CBM, these criteria have been manually optimized. These selection-cuts have been selected to maximize the signal to background ratio (S/B) of the $\\Lambda$ for a certain energy on a collisions generator. The selection criteria mainly depends on the collision energy, decay channel and detector configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually selected cuts\n",
    "manCut_loverdl = 30\n",
    "manCut_dca = 0.4\n",
    "manCut_chi2topo = 20\n",
    "manCut_chi2geo = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQrDUlyr_07i"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',fontsize = 15)\n",
    "    plt.xlabel('Predicted label',fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 636
    },
    "id": "A0CXPHGR72uS",
    "outputId": "c94f8fd1-0499-4f48-d284-4c639b5f7462"
   },
   "outputs": [],
   "source": [
    "new_check_set= df_clean.copy()\n",
    "new_check_set['new_signal']=0\n",
    "\n",
    "mask1 = (new_check_set['loverdl'] > manCut_loverdl) & (new_check_set['distance'] < manCut_dca)\n",
    "\n",
    "mask2 = (new_check_set['chi2topo'] < manCut_chi2topo) & (new_check_set['chi2geo'] < manCut_chi2geo) \n",
    "\n",
    "new_check_set = new_check_set[(mask1) & (mask2)] \n",
    "\n",
    "#After all these cuts, what is left is considered as signal, so we replace all the values in the 'new_signal'\n",
    "# column by 1\n",
    "new_check_set['new_signal'] = 1\n",
    "cnf_matrix1 = confusion_matrix(new_check_set['issignal'], new_check_set['new_signal'], labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "fig, axs = plt.subplots(figsize=(10, 8))\n",
    "axs.yaxis.set_label_coords(-0.04,.5)\n",
    "axs.xaxis.set_label_coords(0.5,-.005)\n",
    "plot_confusion_matrix(cnf_matrix1, classes=['signal','background'], title='Confusion Matrix for manually set cuts')\n",
    "fig.savefig(directory+'img/xgb/confusion_matrix_for_manually_set_cuts.pdf')\n",
    "fig.savefig(directory+'img/xgb/confusion_matrix_for_manually_set_cuts.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confustion_stats(df_clean, cnf_matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ncr9MQSdD7WD"
   },
   "outputs": [],
   "source": [
    "cut3 = test_best\n",
    "mask1 = df_clean['xgb_preds']>cut3\n",
    "df3=df_clean[mask1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 654
    },
    "id": "DwAD5im6lIj4",
    "outputId": "3660034f-8f9b-472b-aced-35746f9ad1ee"
   },
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "range1= (lowerMassCut, upperMassCut)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 1,figsize=(15,10), sharex=True,  gridspec_kw={'width_ratios': [10],\n",
    "                           'height_ratios': [8,4]})\n",
    "\n",
    "ns, bins, patches=axs[0].hist((df3['mass']),bins = 300, range=range1, facecolor='red',alpha = 0.3)\n",
    "ns1, bins1, patches1=axs[0].hist((new_check_set['mass']),bins = 300, range=range1,facecolor='blue',alpha = 0.3)\n",
    "#plt.xlabel(\"Mass in GeV\", fontsize = 15)\n",
    "axs[0].set_ylabel(\"counts\", fontsize = 15)\n",
    "#axs[0].grid()\n",
    "axs[0].legend(('XGBoost Selected K-short','KFPF selected K-short'), fontsize = 15, loc='upper right')\n",
    "\n",
    "#plt.rcParams[\"legend.loc\"] = 'upper right'\n",
    "axs[0].set_title(\"The K-short Invariant Mass histogram with KFPF and XGB selection criteria on KFPF variables\", fontsize = 15)\n",
    "axs[0].grid()\n",
    "axs[0].tick_params(axis='both', which='major', labelsize=15)\n",
    "#fig.savefig(\"whole_sample_invmass_with_ML.png\")\n",
    "\n",
    "\n",
    "hist1, bin_edges1 = np.histogram(df3['mass'],range=(lowerMassCut, upperMassCut), bins=300)\n",
    "hist2, bin_edges2 = np.histogram(new_check_set['mass'],range=(lowerMassCut, upperMassCut), bins=300)\n",
    "\n",
    "#makes sense to have only positive values \n",
    "diff = (hist1 - hist2)\n",
    "axs[1].bar(bins[:-1],     # this is what makes it comparable\n",
    "        ns / ns1, # maybe check for div-by-zero!\n",
    "        width=0.001)\n",
    "plt.xlabel(\"Mass in $\\dfrac{GeV}{c^2}$\", fontsize = 15)\n",
    "axs[1].set_ylabel(\"XGB / KFPF\", fontsize = 15)\n",
    "axs[1].grid()\n",
    "axs[1].tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "plt.show()\n",
    "fig.tight_layout()\n",
    "fig.savefig(directory+'img/xgb/kaon_inv_mass_comparison.pdf')\n",
    "fig.savefig(directory+'img/xgb/kaon_inv_mass_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LZLqe9vwWoY",
    "outputId": "89f0f934-cd7a-4195-fe2a-113e85f2c7c3"
   },
   "outputs": [],
   "source": [
    "#del x,y,x_test,y_test,x_whole,y_whole,dtest,dtrain,dtest1,df3,df_clean,df_scaled\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4qfipIF1f9C"
   },
   "source": [
    "# Importing the final predictor to root\n",
    "We will use the treelite library to transport the final predictor of our model to root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EclXZNYd11Dc",
    "outputId": "0e3ba6c8-0ba4-4d77-fbd2-31beff07fcda"
   },
   "outputs": [],
   "source": [
    "#install treelite \n",
    "import treelite\n",
    "#create an object out of your model, bst in our case\n",
    "model = treelite.Model.from_xgboost(bst)\n",
    "#use GCC compiler\n",
    "toolchain = 'gcc'\n",
    "#parallel_comp can be changed upto as many processors as one have\n",
    "model.export_lib(toolchain=toolchain, libpath='./mymodel.so',\n",
    "                 params={'parallel_comp': 8}, verbose=True)\n",
    "\n",
    "# Operating system of the target machine\n",
    "platform = 'unix'\n",
    "# C compiler to use to compile prediction code on the target machine\n",
    "toolchain = 'gcc'\n",
    "# Save the source package as a zip archive named mymodel.zip\n",
    "# Later, we'll use this package to produce the library mymodel.so.\n",
    "model.export_srcpkg(platform=platform, toolchain=toolchain,\n",
    "                    pkgpath='./mymodel.zip', libname='mymodel.so',\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ha8m3Kub6UAt",
    "outputId": "8524aa6c-7c50-45ae-a3cf-42d9b8ab1cab"
   },
   "outputs": [],
   "source": [
    "!unzip mymodel.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o4DPuQAQAGVS",
    "outputId": "e25d713f-3048-4a04-b5d1-8352c7394609"
   },
   "outputs": [],
   "source": [
    "#Build the source package (using GNU Make or NMake).\n",
    "%cd mymodel \n",
    "!make\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXGt5x8fpZBu"
   },
   "source": [
    "If one wants to transfer this model to a different computer, target machine, then one should follow the following commands in command prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VoJldomSp-bu"
   },
   "outputs": [],
   "source": [
    "sftp john@lxpool.gsi.de\n",
    "sftp> put mymodel.zip\n",
    "sftp> quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjOvqUdRqNvA"
   },
   "outputs": [],
   "source": [
    "ssh john@lxpool.gsi.de\n",
    "unzip mymodel.zip\n",
    "cd mymodel\n",
    "make -j8\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpKXRlds6jpD"
   },
   "source": [
    "The following C++ code can be the applied as a macro on some new data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4pYC2l_r6iuO",
    "outputId": "025d02e1-2310-4156-ea1e-6a9904498907"
   },
   "outputs": [],
   "source": [
    "%%writefile test.cpp\n",
    "#include \"TROOT.h\"\n",
    "#include \"TFile.h\"\n",
    "#include \"TTree.h\"\n",
    "#include \"TH1F.h\"\n",
    "#include <vector>\n",
    "#include \"TString.h\"\n",
    "#include \"TMath.h\"\n",
    "#include \"TH1.h\"\n",
    "#include \"TF1.h\"\n",
    "#include \"TCanvas.h\"\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include \"main.c\"\n",
    "\n",
    "\n",
    "\n",
    "void python_to_root(){\n",
    "  TCanvas* c1 = new TCanvas();\n",
    "  //c1->Divide(2,1);\n",
    "  \n",
    "  //c1->cd(1);\n",
    "  \n",
    "  /* Open the root file*/\n",
    "  TFile *f = new TFile(\"/gdrive/My Drive/presentations/treelite/10k_events_PFSimplePlainTree.root\",\"UPDATE\");\n",
    "  TTree *t1 = (TTree*)f->Get(\"PlainTree\");\n",
    "  \n",
    "  \n",
    "  Float_t  LambdaCandidates_chi2primneg, LambdaCandidates_chi2primpos, LambdaCandidates_ldl, LambdaCandidates_distance, LambdaCandidates_chi2geo, LambdaCandidates_mass;\n",
    "  \n",
    "  t1->SetBranchAddress(\"LambdaCandidates_chi2primneg\",&LambdaCandidates_chi2primneg);\n",
    "  t1->SetBranchAddress(\"LambdaCandidates_chi2primpos\",&LambdaCandidates_chi2primpos);\n",
    "  t1->SetBranchAddress(\"LambdaCandidates_ldl\",&LambdaCandidates_ldl);\n",
    "  t1->SetBranchAddress(\"LambdaCandidates_distance\",&LambdaCandidates_distance);\n",
    "  t1->SetBranchAddress(\"LambdaCandidates_chi2geo\",&LambdaCandidates_chi2geo);\n",
    "  t1->SetBranchAddress(\"LambdaCandidates_mass\",&LambdaCandidates_mass);\n",
    "  \n",
    "  \n",
    "  std::vector<float> output{};\n",
    "  \n",
    "  const long n_entries = t1->GetEntries();\n",
    "  output.reserve(n_entries);\n",
    "\n",
    "  const size_t n_features = 5;\n",
    "  union Entry input[n_features];\n",
    "  \n",
    "  //let's create a new branch which will store the probability returened for each proton-pion pair by our model \n",
    "  Float_t probab;\n",
    "  TBranch *bpt = t1->Branch(\"probab\",&probab,\"probabilities\");\n",
    "  t1->SetBranchAddress(\"probab\",&probab);\n",
    "  \n",
    "  TH1F *h = new TH1F(\"h_prob\", \"This is the #Lambda invariant mass after the cut >0.918080\", 300, 1.04, 1.4);\n",
    "  h->GetXaxis()->SetTitle(\"[GeV]\");\n",
    "  \n",
    "  for (long i = 0; i < n_entries; i++)\n",
    "   {\n",
    "     t1->GetEntry(i);\n",
    "    \n",
    "    input[0].fvalue = LambdaCandidates_chi2primneg;\n",
    "    input[1].fvalue = LambdaCandidates_chi2primpos;\n",
    "    input[2].fvalue = LambdaCandidates_ldl;\n",
    "    input[3].fvalue = LambdaCandidates_distance;\n",
    "    input[4].fvalue = LambdaCandidates_chi2geo;\n",
    "    \n",
    "    output.push_back(predict(input, 0));\n",
    "    probab= output.at(i);\n",
    "   \n",
    "    if(probab > 0.918080){\n",
    "    h->Fill(LambdaCandidates_mass);}\n",
    "  }\n",
    "  \n",
    "  h->Draw();\n",
    "\n",
    "  \n",
    "  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjofU-aMx8O7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nkyy2xmSxb2g",
    "outputId": "f6dfac3c-a3b8-47ce-9d0b-d8638f708199"
   },
   "outputs": [],
   "source": [
    "# Installing root\n",
    "!mkdir -p APPS\n",
    "!pwd\n",
    "!cd APPS && wget https://root.cern/download/root_v6.22.00.Linux-ubuntu19-x86_64-gcc9.2.tar.gz \n",
    "!cd APPS && tar -xf root_v6.22.00.Linux-ubuntu19-x86_64-gcc9.2.tar.gz"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of XGB for Collaboration Meeting clean.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
